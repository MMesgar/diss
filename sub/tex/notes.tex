%%%%%%%%%%%%%%%%%%5
%% Into
%
As an example, consider the following text snippet\footnote{Taken from \url{https://www.biography.com/people/albert-einstein-9285408}, accessed 28 May 2018.}.

\begin{examples}
Albert Einstein grew up in a secular Jewish family. 
His father, Hermann Einstein, was a salesman and engineer who, with his brother, founded Elektrotechnische Fabrik J.\ Einstein \& Cie, a Munich-based company that manufactured electrical equipment. 
Albert’s mother, the former Pauline Koch, ran the family household. 
Einstein had one sister, Maja, born two years after him.
\end{examples}
All of these sentences are hanged together by talking about Albert Einstein's family. 
The first sentence of this text is a general introductory about the family of Albert Einstein. 
The rest of sentences give more specific descriptions of his family members. 

You will say that you want to rank these two texts with respect to their coherence. 

%%%%%%%%%%%%%%%%%%%%
%%%%% Related Work
%%%%%%

\subsection{Other Applications and Evaluation Methods}

In order to complete the discussion about coherence applications and evaluation methods, 
in this part of the thesis we review other related methods in the literature. 

\paragraph{Sentence ordering.} 
The flow of information presented in coherent texts is easy to follow \cite{lapata03,barzilay04,karamanis04b,barzilay05a,soricut06}. 
As it is explained in Chapter \ref{ch:coherence}, a text is beyond a random sequence of sentences. 
The order of sentences affects how difficult information presented in a text can be understood.  
Given the above explanation, a text can be taken as an unordered bag of sentences and an evaluation task for a coherence model would be to find the perfect order of sentences that maximizes the coherence of the text.  
Unfortunately, finding the best ordering is NP-complete \cite{?} and non-approximative \cite{althaus04}.
Therefore, the sentence ordering task needs to be simplified for being used as a coherence evaluation method. 
Two restricted versions of this task are the discrimination task, and the insertion task. 
In both subtasks the model is examined to check whether it can distinguish between the correct (original) order of sentences in a document and an incorrect (non-original) one. 


The discrimination task is based on this assumption that the original order of sentences in a text is taken as the best order and any scrambling of sentences disturbs the perceived coherence of the text. 
By rearranging the order of sentences in a text, the text becomes more difficult to understand or less coherent because the order of presented information is disturbed. 
A coherence model ideally ranks the original order of sentences higher than any other permutations. 
The permutation task is easy to solve \cite{elsner11b}, especially for documents with many sentences, because permuting whole sentences yields a version of a text which is very different, and therefore easier to distinguish, from the original text. 

The insertion task \cite{barzilay05a}, similar to the discrimination task, is designed to evaluate the ability of a coherence model in distinguishing the proper order of sentences in a text from other orders. 
However, the insertion task provides permutations that are not very different with the original text. 
The intuition is that if one sentence of a text is removed from the text, a gap is created between sentences which disturbs the perceived coherence of the text.  
The removed sentence can be inserted in between of any two sentences in the text and creates a permutation of the original text.   
Given a document with $n$ sentences, if the sentence at position $i$ is removed then there are $n$ possible positions, including the position $i$, to reinsert this sentence and create a permutation. 
Table \ref{tab:insertion_task} shows an example of removing the sentence in position $0$ ($\underline{A}$) of a document with four sentences, $\left \{ A, B, C, D \right \}$, and reinserting the sentence in all possible positions. 

\begin{table}[!ht]
	\begin{center}
		\begin{tabular}{cc|cccc}
		\hline
		Position & Original order  & \multicolumn{4}{c}{Permutations}\\
		\hline
		$0$ & $\underline{A}$ & $\underline{A}$ & $B$ & $B$ & $B$\\
		$1$ & $B$ 			  & $B$  & $\underline{A}$ & $C$ & $C$\\
		$2$ & $C$			  & $C$ & $C$ & $\underline{A}$ & $D$\\
		$3$ & $D$			  & $D$ & $D$ & $D$ & $\underline{A}$\\
		\end{tabular}
	\end{center}
	\caption{An illustration of the insertion task. } 
	\label{table:insertion_task}  
\end{table}

The insertion task differs with the permutation task in three properties. 
First, in the insertion task a text with $n$ sentences is evaluated exactly in $n^2$ time, which, though it can be significant, is still much faster than ordering. 
Second, it is more difficult for texts with many sentences to solve this task, because permutations only differ by one sentence.  
Third, in the insertion task a text is compared with many more permutations.  
 
In general, both sentence ordering tasks are artificial. 
Texts are generated ..... 

\paragraph{Essay Scoring} 

Higgins et al. ("Evaluating multiple aspects of coherence in student essays") evaluate multiple aspects of coherence in essays by defining some features based on semantic similarity measures and discourse structures. 
Some features include the relatedness between the essay and the topic and some other capture the relatedness between discourse elements (e.g. intra-sentential quality and sentence-relatedness within discourse segments. 
In earlier work, Folz et al. (1998) and Wiemer-Hastings and Graesser (2000) have developed coherence aspects of student writing.
Their system measures lexical relatedness between text segments by using vector-based similarity between adjacent sentences.
This linear approach to similarity scoring is in line with the TextTiling scheme (Hearst and Plaunt, 1993; Hearst 1997), which may be used to identify the subtopic structure of student essays. 
Miltsakaki and Kukich (2000) have also addressed the issue of establishing coherence of student essays, using the Rough shift element of Centering Theory. 
Higgins et al. () adopted a vector-based method of semantic representation: Random Indexing (Kanerva et al. 2000; Sahlgren, 2001) while other work use Latent Semantic Analysis as a semantic similarity measure. 
Their output shows that the semantic similarity between sentences is not as promising as other features. 
It relatively is rare to find a sentence which is not related to anything in the same discourse. 
\newcite{benguosheng13} propose a bilingual lexical cohesion model for document-level machine translation.  
The coherence of the translated document is achieved by considering lexical cohesion items in the source document. 
The idea is that the strength of the semantic relation between two words should be retained in their counterparts in the translated document. 
In this way the translated document is easier to read if the source document is readable. 
\newcite{wongbillytm12} integrate lexical cohesion in assessment of translated documents. 
Lexical cohesion items in a translated document should be ideally similar to those in its associated human translation. 
In a closely related study, \newcite{fenglijun09} use lexical chains to measure readability. 
Lexical chain features are employed to indicate the number of entities/concepts that a reader must keep in mind while reading a document.  
\newcite{flor13} present a coherence model based on lexical tightness among content words in a text. 
Lexical tightness represents the degree to which a text tends to use words that are highly inter-associated in a language. 
They show that lexical tightness strongly correlates with readability level of expertly rated reading materials. 

\newcite{burstein10} use the same scoring function that is used by \newcite{barzilay05}. 
The main contrinution of this paper is that they collect some some essay data and ask human annotators to read it and classify them into two category low- and high- coherence. 


% Webber et al. (NLE 2012, Discourse structure and language technology):



#####################################################################################
\cite{todirascu16}
#####################################################################################

Although psycholinguistic experiments have shown that a higher level of cohesion and coherence between a pair of
related sentences decreases their reading time (Kintsch et al., 1975; Mason and Just, 2004), the added
value of these textual dimensions for readability models (compared to traditional features) remains unclear, 

Coherence is defined as a “semantic property of discourse, based on the interpretation of each individual
sentence relative to the interpretation of other sentences” (Van Dijk, 1977, 93)

Cohesion is a property of text represented by explicit formal grammatical ties (discourse connectives)
and lexical ties that signal how utterances or larger text parts are related to each other.

In this article, we consider explicit lexical ties such as anaphoric, co-reference and lexical chains as
cohesive features. We study the correlation between these cohesive features and text complexity


#####################################################################################
\cite{petersencasper15}
#####################################################################################

Good definition of coherence and three levels of coherence: intentional, attentional, organizational 

Good related work
The idea of more entity in a sentence meaning less coherent text

Conveying the message that outdegree is not the best for the entity graph. They computed other graph properties.

Evaluation on 
	the sentence ordering task
	Information Retrieval task (TREC): as post processing re-rank the retrieved documents

Based on this paper: our model is a coherence model that captures local and global coherence.

Entity-based models are based on the Centering theory.

The model is unsupervised.

Idea: using information entropy: information entropy is the expected value of the information content of a random variable. coherence score is the inverse of the entropy of entities

A connection between probability of the entities and normalization method of the normalized entity graph. Another connection is Entity distance and Adjacent Topic flow.

Check the highlighted references for further readings; they are good papers for coherence.

#####################################################################################
 \cite{} -- Micha Elsner's Thesis
#####################################################################################
At a high level, a coherent document discusses a sequence of topics in a structured way, with each topic
tending to occupy a single segment of the text (Hearst, 1997; Galley et al., 2003).


Inside each segment, adjacent sentences link to one another in a local structure, in which they share
not just a general topic but some specific logical relationship.

Local coherence models fall into several types: rhetorical models try to explicitly describe the way propositions link up to form an argument or narrative. 
Lexical models capture the tendency for adjacent sentences to use similar vocabulary, since they
express related propositions. 
Entity-based models look at the way entities objects existing in the world are
mentioned in the discourse.

Marcu (1997) and Mellish et al. (1998), Both these papers focus on text generation; they take
input in a non-linguistic format where the relationships between propositions are made explicit. 
For texts that may or may not be coherent in the first place (such as those formed
by extracting sentences from a multidocument collection), this problem is likely to be even worse. Pitler
and Nenkova (2008) obtain some use from rhetorical structure in predicting readability for human-authored
documents where rhetorical relations were marked by hand, suggesting that this type of information would
indeed be useful if it were available.

Lexical features, on the other hand, require no annotation at all; they are accessible directly from the text.
However, their expressive power is limited; in general, lexical methods aim to measure similarity between
pairs of sentences adjacent in a text, with little ability to predict the direction of the relationship or how it
relates to a higher-level structure. Basic lexical methods restrict themselves to counting repeated words; a
variety of metrics are covered in Lapata and Barzilay (2005). Information retrieval techniques, most notably
TF-IDF (Jones, 1972), can be used to approximate the importance of each word. More sophisticated models
aim to learn associations between different words (for instance, car is a good context for tire, or brakes
(Prince, 1981)). Foltz et al. (1998) uses a separate dimensionality reduction phase, LSA (Deerwester et al.,
1990) to learn words that are similar to one another. Lapata (2003) is the rst to learn word associations for
local coherence directly, in a manner that respects ordering (so it is able to learn that ``tire" is not as good a
context for ''car" as vice versa). Soricut and Marcu (2006) uses IBM model 1 (Brown et al., 1993) to learn
these word associations.
Work on lexical chains searches for larger-scale groups of related words which span multiple sentences;
rather than relating each sentence to the next, they build larger structures, chains of related words which
persist throughout a topical segment. As with other lexical methods, lexical chains require some basic method
for finding significant repeated or similar words. Galley et al. (2003) uses TF-IDF; Morris and Hirst (1991)
describes a variety of methods based on thesauri.

The original motivation for computational coherence modeling was text planning; Marcu (1997) points out
that a system attempting to explain a set of facts would require a way of ordering those facts, and that this
ordering should result in coherent transitions between adjacent units of text.
Jing and McKeown
(1999) nds that human summarizers often reorder sentences to create more coherent output, suggesting that
poor ordering can compromise readers' understanding; Barzilay et al. (2002) demonstrates this empirically,
as does Lapata (2006). Barzilay et al. (2002) also observes that imposing a coherent ordering on the resulting
set of sentences is non-trivial, motivating the use of coherence models to solve the problem. 

Barzilay et al. (2002) use chronological information and topical clustering; Lapata (2003) learns word-to-word associations that typify coherent transitions;
Lapata and Barzilay (2005) and Barzilay and Lapata (2005) use a syntactic model of information structure.

In addition to text ordering, local coherence models have also been used to score the uency of texts 
written by humans or produced by machine. Evaluation of human-authored text can characterize how difcult
it is to read. The rst use of coherence modeling in this context is Foltz et al. (1998), who use similarity
between adjacent sentences to measure readability of encyclopedia articles and a historical text. 

Pitler and Nenkova (2008) models readers' reported difculty in reading WSJ news articles. 
Scores for
human-authored text are also useful in an educational context, since they can augment human grading for
student essays. Miltsakaki and Kukich (2004), Higgins et al. (2004) and Burstein et al. (2010) model various
aspects of coherence and nd that they are predictive of the grades eventually assigned.
coherence modeling is an implicit ingredient in many attempts to understand the hidden structure
of text. 

%This article focuses on
%local coherence, which captures text relatedness at the level of sentence-to sentence transitions. 
%Local coherence is undoubtedly necessary for global coherence and has received considerable attention in computational linguistics (Foltz,
%Kintsch, and Landauer 1998; Marcu 2000; Lapata 2003; Althaus, Karamanis, and Koller2004;Karamanisetal.2004

%The entity grid, however, does not commit itself to fine-grained rankings of the centers or to a pre-specified transition rule. Instead, it provides a representation which can be constructed from relatively light-weight syntactic analyses, and in which the kinds of rules specified by the theory will be efficiently learnable.

%Lapata (2003) gives the first model for learning word associations for coherence from data.
%Her model gives the probability of a sentence W with words $w_i$ following a sentence V with words $V_j$ as:
%$P(W) = \prod_{i} \prod_{j}p(w_i|v_j)$
%This model explains each word w as produced by a predecessor word v, although which word in the previous
%sentence should serve as predecessor for each w is unknown.
%
%Soricut and Marcu (2006), inspired by a personal communication from Kevin Knight, use IBM model 1
%(Brown et al., 1993) to learn word-to-word associations.
%
%IBM uses hidden variables to obtain a true generative
%model with the same lexical parameterization as Lapata (2003).
%
%IBM solves the problem that we do not know the correct predecessor word vj for each wi by introducing
%a hidden alignment variable ai 2 [0::j] which indicates precisely which previous word is responsible for each
%current word.
%
%Because IBM-1 has hidden variables, its parameters cannot be learned by direct estimation; conventionally,
%EM or variational EM are used instead.
%
%Elsner's implementation of IBM-1 produces and conditions on nouns and verbs only;
%
#####################################################################################
\cite{Barzilay} Modeling Local Coherence: An Entity-Based Approach
#####################################################################################
Linguistic Modeling.
Entity-based accounts of local coherence have a long tradition
within the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Halliday
and Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi,
and Weinstein 1995). A unifying assumption underlying different approaches is that
discourse coherence is achieved in view of the way discourse entities are introduced
and discussed.This observation is commonly formalized by devising constraints on the
linguistic realization and distribution of discourse entities in coherent texts.



There is a research tradition developing metrics for readability and using these metrics to quantify how difficult it is to understand a
document. 
Shallow features such as word, sentence and text length, which only capture superficial properties of a text, have been used traditionally \cite{flesch48,kincaid75}. 
\newcite{declercq14} use traditional shallow features and apply these to a new corpus annotated with two different methodologies. 
However, some studies indicate that shallow features do not precisely predict the readability of a text \cite{fenglijun09,petersen09}. Later studies introduce deeper (more
semantic) features such as those obtained by language models \cite{siluo01,collins-thompson04} and syntactic features like the number of NPs in sentences or the height of the sentence's parse tree \cite{schwarm05,heilman07}. 
 \newcite{barzilay08} propose an entity-based coherence model which operationalizes some of the intuitions behind the centering model \cite{grosz95}.  
 Although this model works well on the sentence ordering and summary coherence rating tasks, it does not work well for readability assessment. 
 Only when combining the entity grid with features taken from \newcite{schwarm05} the entity grid performs competitively.

While most of these studies predict the readability level of documents, \newcite{pitler08} present a new readability dataset with \textit{Wall Street Journal} articles, where each article is assigned human readability ratings. 
They analyze the correlation between different readability features and human readability scores. 

\newcite{miltsakaki00} show that for having a well-written text, the author should avoid to concatenate sentences without sharing any entity.


There are various approaches for summarizing scientific articles. Citations have been used by many researchers for summarization in this domain \cite{elkiss08,mohammad09,qazvinian08,abu-jbara11}. \newcite{nanba00} develop rules for categorizing citations by analyzing the citation sentences. \newcite{newmanmark01} analyzes the structure using a citation network. Similarly, \newcite{siddharthan07} discover scientific attributions using citations. Discourse structure (but not necessarily coherence) has been used by \newcite{teufel02}, \newcite{liakata13} and others for summarizing scientific articles.


Until now, only few works have considered coherence while summarizing scientific articles. \newcite{abu-jbara11} work on citation based summarization. They preprocess the citation sentences to filter out irrelevant sentences or sentence fragments, then extract sentences for the summary. Eventually, they refine the summary sentences to improve readability.

There are few methods \cite{hirao13,parveen15a,gorinski15} which integrate coherence in optimization.
These methods do not take into account the overall structure of the summary. Unlike earlier methods, we incorporate coherence patterns in optimization.


While cohesiveness is vital for coherence, too much repetition of the same word can, in fact, harm the discourse quality (Witte and Faigley, 1981). 


It is also known in discourse parsing a sentence is attached to its immediate preceding sentence (Pelszus and Sted, EMNLP 2015). 


(Pelszus and Sted, EMNLP 2015)'s study about the argumentation mining shows that the structure over text segments is beyond linear and it's mainly a graph. They limit this structure to a tree, though.
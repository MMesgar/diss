% for sublime text 3
%!TEX root = diss.tex

\chapter{Introduction}
\label{ch:intro}

Research in natural language processing intends to provide models for understanding and generating texts.   
One crucial aspect in the processing of multi-sentence texts is \emph{coherence}: How sentences in such a text are related to one another to make the text as a whole. 
As an example, consider the following text snippet\footnote{Article \emph{D31038.M.100.T.B} Taken from \url{http://people.csail.mit.edu/regina/coherence/} accessed 28 May 2018. 
%\url{file:///Users/mohsenmesgar/Downloads/sum-docs/D31038.M.100.T.B.html}, accessed 28 May 2018.
}:

\begin{examples}
\label{ex:high-coh-text}
A total of 248 people, including a dozen Americans, were killed in the terrorist bombing of the U.S.\ Embassy in Nairobi on August 7, 1998. 
A twin attack on the U.S.\ Embassy in Tanzania killed 11 people, all Africans. 
Osama bin Laden is the suspected mastermind of the bombings. 
Through the Saudis, the United States asked the Taliban, the Islamic movement that controls most of Afghanistan, to deport bin Laden, but they refused. 
Evidence suggests that the terror suspects accused in the bombings, regardless of their nationality or place of residence, are associates of bin Laden or associated with terrorist groups under his control.
\end{examples}

The above text is a summary which is provided by a human from several documents. 
All sentences are attached together in a way that the whole text transmits a meaning. 
The first sentence gives some information about ``bombing''. 
The second sentence takes this information and expands it to another instance. 
The third sentence uses the given information (``bombing'') in its preceding sentences to introduce ``Osama bin Laden'' as new information. 
The rest of the sentences follow a similar structure of relationships.  
The text below\footnote{Article \emph{D31038.M.100.T.16} taken from 
\url{http://people.csail.mit.edu/regina/coherence/}
%\url{file:///Users/mohsenmesgar/Downloads/sum-docs/D31038.M.100.T.16.html}, 
accessed 28 May 2018.} is a summary which is generated automatically from the same cluster of documents. 
It is less coherent than the text in Example \ref{ex:high-coh-text}, as its sentences are weakly related to each other. 

\begin{examples}
\label{ex:low-coh-text}
Solemn-faced Kenyans, whose relatives were killed in the terrorist bombing of a U.S.\ Embassy, collected benefits on Friday. 
They said failed to compensate for their losses. 
Nearly two months after the bombings of the American Embassies in Kenya and Tanzania, a picture of those charged in the case is slowly emerging. 
Nine months before the attack on the American Embassy here, U.S.\ intelligence officials received a detailed warning that Islamic radicals were plotting to blow up the building, according to Kenyan and American officials. 
\end{examples}

A coherence model should first represent how sentences in a text are related to each other. 
It then uses the structure of relations to ideally rank and distinguish texts with respect to their perceived coherence.  
For example, consider the text snippets in Example \ref{ex:high-coh-text} and Example \ref{ex:low-coh-text}, a coherence model should ideally rank the former text higher, in terms of coherence, than the latter one. 

As it has been shown in the above examples, applications of a coherence model are in downstream tasks in natural language processing. 
One example is in readability assessment, in which coherence is employed as an essential factor in measuring the quality of texts. 
Coherent texts avoid confusion, so they are easy to read and follow. 
Another example is in text summarization, which can employ a coherence model in two ways:   
First, a coherence model can be used for evaluating the quality of outputs of automatic summarizers. 
In this case, the usage of coherence models for the summarization task is similar to their usage in general text quality assessment. 
Second, a coherence model can as one component be integrated into summarization systems to generate coherent summaries directly. 

In the research presented in this dissertation, we aim to develop a computational model for text coherence. 
We also intend to evaluate our coherence model in extrinsic applications. 
In the remainder of this chapter, we take a further look into the motivation of the research conducted in this thesis and formulate main research questions (Section \ref{sec:intro-motivation}), briefly explain our contributions (Section \ref{sec:intro-contributions}), present the outline of this dissertation (Section \ref{sec:intro-outline}), and describe which parts of this dissertation were published (Section \ref{sec:intro-published}). 

\section{Motivation and Research Questions}
\label{sec:intro-motivation}

As we have described above, the goal of computational coherence models is to compare texts with respect to their coherence. 
It suggests that there should be certain features which are characteristic of coherent texts while these features are absent in incoherent texts. 
In the literature, we encounter different sets of features relying on relations which are extracted from a text. 
One type of relations prominently employed by coherence models is coreference. 
In such coherence models, relations among sentences represent the existence of noun phrases that refer to the same entity in sentences. 
The underlying premise of these models is that coherent texts reveal specific patterns in their relations. 
However, these models predefine and limit patterns to linear relations over adjacent sentences. 
This observation leads us to the first research question investigated in this dissertation: 
\textbf{Do there exist nonlinear connectivity patterns in coherent texts that take long-distance relations into account?} 
If we can answer this question by discovering frequent patterns, which involve long-distance relations, in coherent texts, a follow-up question arises, which is how the frequencies of these patterns correlate with the quality of texts. 
Another question is whether these features improve the performance of downstream natural language processing systems.  
The answers to these questions help to learn how coherence patterns proposed in the research in this dissertation compete with their peers, where they are evaluated in coherence related downstream tasks. 

In order to develop a robust computational coherence model, we not only need an approach to extract coherence patterns, which represent connectivity structures of sentences, but we also require a computational method to encode semantic relations between sentences. 
Sentence relations are not limited to coreference between referring expressions in sentences; other semantic relations such as synonymy and antonymy among words in sentences can cohere sentences as well. 
This fact motivates our second research question: 
\textbf{How can we model sentence relations in a text beyond coreference relations over entities by means of semantic relations among words of sentences?} 
In order to answer this question, we first need to define appropriate word representations.  
Word representations should give the model the capability to quantify lexico-semantic relations between words. 
The model is then required to encode connections between sentences based on the words that are semantically related. 
Finally, given such representations of relations among sentences in a text, we can use our approach to coherence pattern extraction for modeling the connectivity structure of sentences in texts.
By such patterns, we then rank texts concerning their coherence. 

\section{Contributions}
\label{sec:intro-contributions}

We answer the first question by introducing a graph-based representation of coherence patterns. 
The graph representation empowers our coherence model to take long-distance relations as well as relations among adjacent sentences into account. 
It further captures the connectivity style of relations among sentences. 
In such graphs, nodes encode sentences and edges capture relationships between sentences. 
Then, we formulate the task of extracting coherence patterns from a set of texts as a subgraph mining problem from a set of graphs. 
We show how frequencies of subgraphs in a graph capture the connectivity style of nodes in the graph and consequently the coherence of the corresponding text. 
We illustrate how frequencies of patterns in texts correlate with the quality ratings that are assigned to those texts by human judges. 

We answer the second question by motivating and developing an approach to coherence modeling based on lexical relations. 
This approach represents relations among sentences via a graph. 
Nodes in such graphs represent sentences and edges represent the existence of lexico-semantic relations among words in sentences. 
We explain how word embeddings are employed to quantify the strength of semantic relations between words in sentences. 
We show that applying subgraph mining methods on such graph representations of texts leads to predictive coherence patterns.   
We further investigate the impact of the size of subgraphs on the performance of patterns. 
We discuss that the frequencies of large subgraphs highly correlate with the quality of texts as they are more informative about connectivity structures of graphs in comparison to small subgraphs.
However, most of the large subgraphs only occur in a few graph representations of texts resulting in a sparsity problem.  
We show how smoothing methods, which are applied in statistical language models, can be adapted to solve this sparsity problem in the frequency of coherence patterns. 

The implementation of graph representations, subgraph mining approaches, and the smoothing method discussed in this thesis are publicly available\footnote{Available for download at \url{https://github.com/MMesgar/}}. 

\section{Outline}
\label{sec:intro-outline} 

The remainder of this thesis is organized into six chapters. 

In Chapter~\ref{ch:coherence}, we discuss the task of coherence modeling in detail.  
We give a formal definition of coherence modeling. 
Furthermore, we address linguistic properties, primary issues, and evaluation approaches. 

In Chapter~\ref{ch:rel-work}, we review the related work on which we mainly built our coherence model. 
We survey different tasks that have been employed to evaluate the coherence models presented in the research in this dissertation. 

In Chapter~\ref{ch:coh-patterns}, we present our approach to coherence pattern mining. 
We recast the problem of coherence pattern mining as extracting frequent subgraphs in graph representations of texts. 
We assess the usefulness of coherence patterns on the readability assessment task. 
We show how coherence patterns extracted from a set of news articles correlate with their readability ratings assigned by human judges. 
We observe that the frequencies of patterns as features, which encode coherence, are more predictive than other examined features for ranking texts concerning their coherence. 
A fundamental analysis of the size of extracted subgraphs leads us to the observation that by increasing the number of nodes in subgraphs the predictive power of coherence patterns for ranking texts improves. 
We furthermore evaluate our approach to coherence pattern mining in the summarization task.  
We show how subgraphs extracted from coherent summaries can improve the performance of an automatic summarization system to produce more coherent summaries. 

In Chapter~\ref{ch:lex-graph}, we propose a graph-based representation method for modeling coherence based on lexico-semantic relations between words in sentences.  
We show that coherence patterns extracted from such graphs are more profitable for the readability assessment task in comparison to the patterns extracted from the entity graph representations of texts. 
We investigate broader about the quality of coherence patterns extracted from such lexical graph representations of texts and the influence of their size on the overall performance of the model.   
We explain the sparsity problem in the frequencies of subgraphs and its impacts on the performance of our coherence model. 
Following the smoothing methods utilized in the statistical language models, we introduce an approach to solve the sparsity problem in graphs.  

In Chapter~\ref{ch:conc}, we summarize the answers that the research in this dissertation gives to the research questions formed in Section~\ref{sec:intro-motivation}. 
Furthermore, we discuss possible avenues for future work. 

\section{Published Work}
\label{sec:intro-published}

Most research presented in this thesis is an extension of published research first-authored by the author of this thesis. 
Some parts of the presented research originated from the published research to which the author of this dissertation contributed. 

The idea of using subgraphs as coherence patterns, presented in Chapter~\ref{ch:coh-patterns}, was published in \newcite{mesgar15}. 
A preliminary investigation of graph-based coherence modeling was presented in \newcite{mesgar14}. 
The application of coherence patterns in summarization, also presented in Chapter~\ref{ch:coh-patterns}, is published in \newcite{parveen16}. 
Our lexical approach to local coherence modeling, and the smoothing method, which both are described in Chapter~\ref{ch:lex-graph}, are proposed in \newcite{mesgar16}.
A follow-up paper to the research presented in this dissertation will be published in \newcite{mesgar18}. 



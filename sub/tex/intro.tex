% for sublime text 3
%!TEX root = diss.tex

\chapter{Introduction}
\label{ch:intro}

Research in natural language processing intends to provide models for understanding and generating texts.   
One crucial aspect in processing of multi-sentence texts is \emph{coherence}, how sentences in a text are related to one another to make the text as a whole. 
As an example, consider the following text snippet\footnote{Taken from \url{file:///Users/mohsenmesgar/Downloads/sum-docs/D31038.M.100.T.B.html}, accessed 28 May 2018.}:

\begin{examples}
A total of 248 people, including a dozen Americans, were killed in the terrorist bombing of the U.S.\ Embassy in Nairobi on August 7, 1998. 
A twin attack on the U.S.\ Embassy in Tanzania killed 11 people, all Africans. 
Osama bin Laden is the suspected mastermind of the bombings. 
Through the Saudis, the United States asked the Taliban, the Islamic movement that controls most of Afghanistan, to deport bin Laden, but they refused. 
Evidence suggests that the terror suspects accused in the bombings, regardless of their nationality or place of residence, are associates of bin Laden or associated with terrorist groups under his control.
\end{examples}

This text is a summary of several documents provided by a human. 
All sentences are hanged together such that the whole text transmits a meaning. 
The first sentence gives some information about ``bombing''. 
The second sentence takes this information and expands it to another instance. 
The third sentence uses given information (``bombing'') in its preceding sentences to introduces ``Osama bin Laden'' as new information. 
The rest of sentences follow a similar structure of relations.  
The text below\footnote{Taken from \url{file:///Users/mohsenmesgar/Downloads/sum-docs/D31038.M.100.T.16.html}, accessed 28 May 2018.}, which is an automatically generated summary of the same cluster of documents, is less coherent due to its sentences are weakly related. 

\begin{examples}
Solemn-faced Kenyans, whose relatives were killed in the terrorist bombing of a U.S.\ Embassy, collected benefits on Friday. 
They said failed to compensate for their losses. 
Nearly two months after the bombings of the American Embassies in Kenya and Tanzania, a picture of those charged in the case is slowly emerging. 
Nine months before the attack on the American Embassy here, U.S.\ intelligence officials received a detailed warning that Islamic radicals were plotting to blow up the building, according to Kenyan and American officials. 
\end{examples}

A coherence model should first represent how sentences in a text are related, and then use the structure of relations to ideally rank and distinguish texts with respect to the perceived coherence of texts. 
For example, given the above text snippets a coherence model should rank the first text higher, in terms of coherence, than the second one. 

As it has been shown in the above examples, applications of a coherence model are in downstream tasks in natural language processing. 
One example is readability assessment, in which coherence is employed as an important factor to measure the quality of texts. 
Coherent texts avoid confusions, so they are easy to read and follow. 
Another example is document summarization, which can employ a coherence model in two ways. 
First, as one factor for evaluating outputs of automatic summarizers. 
In this case, the usage of coherence models is similar to text quality assessment. 
Second, as one component of summarization systems in order to generate coherent summaries. 


In the research presented in this thesis we aim to develop computational models of text coherence. 
We also intend to evaluate our coherence model in extrinsic NLP applications. 
In the reminder of this chapter, we further motivate the research conducted in this thesis and formulate main research questions (Section \ref{sec:intro-motivation}), briefly explain our contributions (Section \ref{sec:intro-contributions}), present the outline of this thesis (Section \ref{sec:intro-outline}), and describe which parts of this thesis were published (Section \ref{sec:intro-published}). 

\section{Motivation and Research Questions}
\label{sec:intro-motivation}

As we have described above, the goal of computational coherence models is to compare texts with respect to their coherence. 
This suggests that there should be certain features which are characteristic of coherent texts while those are not found in incoherent texts. 
In the literature we can encounter with different sets of features relying on relations that are discovered from a text. 
One of the prominently employed relations is coreference, where relations among sentences containing noun phrases that refer to the same entity reflect the topics and their points of changes in a text.  
The underlying assumption of these models is that coherent texts reveal certain patterns in their relations. 
However, these models predefine and limit patterns to linear relations over adjacent sentences. 
This observation leads us to the first research question investigated in this thesis: 
\textbf{Do there exist nonlinear connectivity patterns in coherent texts that consider long distant relations into account?} 
If we can answer this question by discovering some frequent patterns in coherent texts, a follow-up question emerges, which is how the frequency of these patterns are correlated with the quality of texts. 
Another natural question is whether these coherence features can be employed by other NLP systems to improve their performance. 
The answer of these questions indicate how our coherence patterns compete with its peers where they are used by downstream tasks. 

In order to develop a powerful computational coherence model, we do not only need an approach to extract patterns representing connectivity structures of sentences, we also require a computational method to encode semantic relations between sentences. 
Sentence relations are not limited to only coreference between referring expressions; other semantic relations between them can cohere sentences as well. 
This motivates our second research question: \textbf{How can we model sentence relations in a text beyond coreference relations over named entities and based on semantic relations between words of sentences?}  
In order to answer this question, we first have to define an appropriate word representation.  
This representation should give the model the capability to quantify lexico-semantic relations between words. 
We then need to encode connections between sentences based on their semantically related words.
Finally, given rich representations of relations among all sentences in a text, we can use our coherence pattern to model the connectivity structure of sentences to rank texts based on the coherence of the text. 

\section{Contributions}
\label{sec:intro-contributions}

We answer the first question by proposing a graph-based representation of coherence patterns. 
The proposed representation enables our coherence model to take long distant relations, as well as relations between adjacent sentences,  into account. 
It also considers the connectivity style of relations among sentences. 
We formulate this by representing a text by a graph whose nodes encode sentences and edges capture relations between sentences. 
Having such graph representations of texts in a corpus leads us to formulate the task of extracting coherence patterns from a set of texts as a subgraph mining problem from a set of graphs in graph theory. 
We show that how frequencies of subgraphs in a graph capture the connectivity style of nodes in the graph and consequently the coherence of the corresponding text. 
We illustrate how frequencies of patterns in texts correlate with quality ratings that are assigned to texts by human judges. 

We answer the second question by motivating and developing an approach to coherence modeling based on lexical relations. 
As before, this approach represents relations among sentences by a graph. 
However, this graph captures any lexico-semantic relations between words of sentences to connect the associated nodes with sentences. 
We explain how embeddings of words are employed to quantify semantic relations between words in sentences. 
We show that applying subgraph mining methods on such graph representation of texts leads to more predictive coherence patterns. 
However, we discuss that although large subgraphs capture more information about connectivity structures of graphs but most of them may occur in few graphs. 
This results a sparsity problem. 
We show how smoothing methods, which are used in statistical language models, can be adapted to solve this sparsity problem in our coherence patterns. 

The implementation of graph representations, subgraph mining approaches, and the smoothing method discussed in this thesis are publicly available\footnote{Available for download at http://.....} 


\section{Outline}
\label{sec:intro-outline}
The reminder of this thesis is organized into ... chapters. 

In Chapter \ref{ch:coherence}, we discuss the task of coherence modeling in detail. 
We give a formal definition and discuss linguistic properties, main issues and evaluation of the task. 

In Chapter \ref{ch:rel-work}, we review related work on that we mainly built our coherence model. 
We survey different tasks that have been used to evaluate coherence models. 

In Chapter \ref{ch:coh-patterns}, we present our approach to coherence pattern mining. 
We show that how coherence patterns extracted from a set of news articles are correlated with human readability ratings assigned by human judges. 
We see our patterns provide more predictive information than other examined coherence features for ranking texts with respect to their coherence. 
A basic analysis of the size of extracted subgraphs in this chapter leads to this observation that larger subgraphs better rank texts for the readability assessment task. 
We furthermore evaluate our coherence patterns in the summarization task.  
we show how subgraphs extracted from coherent summaries can improve the performance of a strong automatic summarization system to produce more coherent texts. 

In Chapter \ref{ch:lex-graph}, we propose our graph-base approach to coherence modeling based on lexical semantic relations between words in sentences. 
We show that coherence patterns extracted from these graphs are more informative for ranking texts for the readability assessment task, because of integrating any semantic relations between sentences into graph representations of texts. 
We explain about the sparsity problem and our approach to solve this problem. 

In Chapter \ref{ch:conc}, we summarize the answers that the research in this gives to the research questions formed in this chapter. 
We discuss possible way for future work. 

\section{Published Work}
\label{sec:intro-published}

Most research presented in this thesis is an extension of published research first-authored by the author of this thesis. 
Some parts of the presented research are based on the published research to which the author of this thesis equally contributed as the first author. 

The idea of using subgraphs as coherence patterns presented in Chapter \ref{ch:coh-patterns} was published in \newcite{mesgar15}. 
A preliminary investigation of graph-based coherence modeling was presented in \newcite{mesgar14}. 
The application of coherence patterns in summarization presented in Chapter \ref{ch:coh-patterns} is published in \newcite{parveen16}. 
Our lexical approach to coherence modeling and the smoothing method proposed in Chapter \ref{ch:lex-graph} are described in \newcite{mesgar16} and employed in \newcite{born17}. 



% for sublime text 3
%!TEX root = diss.tex

\chapter{Related Work}
\label{ch:rel-work}

The task of coherence modeling has received a lot of attention due to its crucial impacts on other natural language processing systems. 
In this dissertation, we propose a novel approach to local coherence modeling based on graph representations of texts. 
We apply our approach to two types of graphs.  
The first type captures coreference relations among entity mentions in a text. 
The second one captures lexical relations among words in a text.  
We evaluate our coherence model by examining its impacts on readability assessment and text summarization. 

Accordingly, we first review entity-based approaches to local coherence (Section \ref{sec:rel-entity-models}). 
We then survey lexical approaches (Section \ref{sec:rel-ent-grid}). 
Finally, we review approaches that use a local coherence model for the readability assessment and summarization tasks (Section \ref{sec:rel-coh-applications}). 

\section{Entity-based Approaches to Local Coherence}
\label{sec:rel-entity-models}

The preliminary steps of the research presented in this dissertation are inspired by entity-based coherence models. 
In this section, we explain the details of two popular entity-based models: the entity grid model and the entity graph model. 
We also discuss their extensions.  

\paragraph{Historical Review.} 



Entity-based approaches to local coherence modeling have a long history within linguistic literature \cite{kuno72,halliday76,prince81a,joshi98}.
Most approaches share a common primary assumption: Coherence is perceived based on how entities are introduced and discussed within a text \cite{barzilay08}. 
Texts that keep referring to similar entities are supposed to be more coherent than those with random and unexpected switches from one entity to another. 
Different linguistic theories support this premise. 
One of them is the centering theory \cite{grosz95,joshi98}, which is discussed in Chapter \ref{ch:coherence}. 

A great deal of research has been devoted to implementing the centering theory \cite{miltsakaki00,karamanis04a} directly. 
However, it is a challenging task because computational models need to determine how to instantiate the parameters of the theory as they are often underspecified. 
Interestingly, \newcite{poesio04b} noticed that even for basic parameters of the centering theory such as ``utterance'', ``realization'', and ``ranking'', multiple interpretations have been developed.
It is because the original theory of centering does not explicitly specify its concepts. 
For example, in some studies entities are ranked with respect to the grammatical role of entity occurrences in a text \cite{brennan87,grosz95} whereas in some other studies entities are ranked with respect to the position of entity mentions in sentences \cite{prince81a} or the familiarity status (the thematic role) \cite{strube.cl99,moens08} of entities.  
Two instantiations of the same theory make different predictions for the same input. 

Another vein of research tries to avoid this by finding an instantiation of parameters so that the parameters are the most consistent with observable data \cite{strube.cl99,karamanis04a,poesio04b}. 
Some others adopt a specific instantiation in a way that the performance of the coherence model improves for a specific task. 
For example, \newcite{miltsakaki00} annotate a corpus of student essays with entity transition information and then show that the distribution of transitions correlates with human grades. 
Analogously, \newcite{hasler03} investigate whether the centering theory can be used in evaluating the readability of summaries, which can be produced by humans or machines, by annotating them with the entity transition information. 
\newcite{poesio04b} demonstrate that the predictive power of the models that directly implement centering theory is highly sensitive to their parameter instantiations, no matter for which task such instantiations are specified. 

\newcite{barzilay05a,barzilay08} propose a general framework for coherence modeling.  
The primary goal of this framework is to eliminate the need for human annotations for parameters in the centering theory, regardless of what the evaluation task is. 
Inspired by that theory, this model hypothesizes that the distribution of entities within coherent texts reveals certain regularities that make these texts recognizable from incoherent ones. 
Machine learning approaches can learn these regularities.  
Since the entity grid model has been the core of many research papers in the area of coherence modeling (including the research presented in this dissertation), we explain details of this model. 

\subsection{The Entity Grid Model}
\label{sec:rel-ent-grid} 

\newcite{barzilay05a,barzilay08} are the first researchers who proposed a general computational approach to local coherence modeling based on the entity relations across adjacent sentences.  
Supported by some linguistic work such as the centering theory \cite{grosz95} and other entity-based theories of text \cite{prince81a}, they assume that the distribution of entities within coherent texts exhibits certain regularities that can be reflected in a grid topology, which is named  ``entity grid''.  
In this dissertation, we refer to this model as the entity grid model because its main idea is to represent the distribution of entities (see Chapter \ref{ch:coherence} for the definition of entity) across sentences in a text via a grid.   
In practice, mentions of an entity are linked together in order to show that they are referring to the same entity. 
Connections between mentions not only show that those are referring to the same entity, but also indicate that sentences that contain those mentions are (almost) about the same topic or information \cite{barzilay08}. 
So entity coreference relations can be seen as signals for local coherence. 

\subsubsection{Text Representation: Entity Grids}

In the entity grid model, each text is represented by a grid, which is a two-dimensional array, whose rows correspond to entities and whose columns match sentences.
An entry $r_{i;j}$ in a grid describes the syntactic role of entity $i$ in sentence $j$ if the entity is mentioned in the sentence. 
The syntactic roles are categorized as subject (S), object (O), or all other syntactic roles (X). 
Besides, if an entity is not mentioned in a sentence, an especial marker (-) fills the corresponding entry $r_{i;j}$ in the grid. 
Finally, if a sentence contains several mentions of one entity, the corresponding entry describes the most important grammatical role of the mentions in the sentence: subject if possible, then object, or finally other. 

The discussion of entity grids develops around two essential questions: Which textual units should be considered mentions of an entity? How should different mentions be linked to represent an entity? 
A perfect solution in this regard would use a coreference resolution system to recognize mentions, to link arbitrary mentions to the same entities, and to discard noun phrases which do not correspond to an entity. 
Since coreference resolution systems are far from perfect\footnote{The highest reported precision of a coreference system on the ? dataset is ?.}, and tend to work even more poorly on incoherent texts, this approach is not generally one utilized.  
Moreover, a non-perfect coreference system introduces more noisy connections to a coherence model than what it fixes \cite{barzilay08}.  
As an alternative, implementations of the entity grid model tend to employ all noun phrases as mentions and apply a heuristic coreference resolution to them.  
This coreference model connects all mentions that have an identical head noun as one entity. 
However, such a coreference resolution system is quite strict and straightforward. 
Detailed discussions of this heuristic are given in \newcite{poesio04c} and \newcite{elsner10}. 

Example~\ref{ex:rel-text} shows a sample text\footnote{The text with ID D31010, taken from the Document Understanding Conference (DUC-2002) dataset, which we use in one of our summarization experiments. Numbers are not marked because they are filtered out in preprocessing.}.  
In this example, noun phrases are marked with brackets as an indication of mentions. 
Mentions in a sentence are associated with their syntactic roles in the sentence, annotated with a letter (S: subject, O: object, and X: others) next to the brackets. 
%/hits/fast/nlp/mesgarmn/Data/ACL13/SummaryCoherence/texts/
%D31010.M.100.T.E.txt. Nubembers (e.g. 29) are filtered out in preprocessing.}  

\begin{examples}
\label{ex:rel-text}
\begin{tabular}{l@{\space}p{12.5cm}}
 $s_0$: &[An arctic cold wave]\textbf{\textsubscript{S}}, [the worst]\textbf{\textsubscript{X}} in [10 years]\textbf{\textsubscript{X}}, hit [parts]\textbf{\textsubscript{O}} of [Europe]\textbf{\textsubscript{X}}, bringing [sub-zero temperatures]\textbf{\textsubscript{O}} and killing [scores]\textbf{\textsubscript{O}} of [people]\textbf{\textsubscript{X}}. \\

 $s_1$: & Hardest hit were [Poland]\textbf{\textsubscript{S}}, [Bulgaria]\textbf{\textsubscript{S}}, and [Romania]\textbf{\textsubscript{S}} as well as [parts]\textbf{\textsubscript{S}} of [central]\textbf{\textsubscript{X}} and [eastern France]\textbf{\textsubscript{X}}. \\

$s_2$: &In [Poland]\textbf{\textsubscript{X}}, [three weeks]\textbf{\textsubscript{X}} of [sub-zero temperatures]\textbf{\textsubscript{X}} killed [at least 85 people]\textbf{\textsubscript{O}} in [November]\textbf{\textsubscript{X}}, 29 more than in [all]\textbf{\textsubscript{X}} of [the previous winter]\textbf{\textsubscript{S}}. \\


$s_3$: &[Most]\textbf{\textsubscript{S}} of [the victims]\textbf{\textsubscript{X}} were homeless [whose deaths]\textbf{\textsubscript{X}} by [exposure]\textbf{\textsubscript{X}} were alcohol related. \\

$s_4$: &[Blizzards]\textbf{\textsubscript{X}} and [cold temperatures]\textbf{\textsubscript{S}} also hit [Bulgaria]\textbf{\textsubscript{X}} and [Romania]\textbf{\textsubscript{O}}, stranding [hundreds]\textbf{\textsubscript{O}} in [their cars]\textbf{\textsubscript{X}}. \\

$s_5$: &Elsewhere, [snow]\textbf{\textsubscript{S}} blanketed [the Italian island]\textbf{\textsubscript{O}} of [Capri]\textbf{\textsubscript{X}} for [the first time]\textbf{\textsubscript{X}} in [10 years]\textbf{\textsubscript{X}}. 
\end{tabular}
\end{examples}
%(ROOT (S (NP (NP (DT An) (JJ arctic) (JJ cold) (NN wave)) (, ,) (NP (NP (DT the) (JJS worst)) (PP (IN in) (NP (CD 10) (NNS years)))) (, ,)) (VP (VBD hit) (NP (NP (NNS parts)) (PP (IN of) (NP (NNP Europe)))) (, ,) (S (VP (VP (VBG bringing) (NP (JJ sub-zero) (NNS temperatures))) (CC and) (VP (VBG killing) (NP (NP (NNS scores)) (PP (IN of) (NP (NNS people)))))))) (. .)))

%(ROOT (S (S (VP (ADVP (RBS Hardest)) (VBN hit))) (VP (VBD were) (NP (NP (NP (NNP Poland)) (, ,) (NP (NNP Bulgaria)) (, ,) (CC and) (NP (NNP Romania))) (CONJP (RB as) (RB well) (IN as)) (NP (NP (NNS parts)) (PP (IN of) (NP (NP (JJ central)) (CC and) (NP (JJ eastern) (NNP France))))))) (. .)))

%(ROOT (S (PP (IN In) (NP (NNP Poland))) (, ,) (NP (NP (CD three) (NNS weeks)) (PP (IN of) (NP (JJ sub-zero) (NNS temperatures)))) (VP (VBD killed) (NP (QP (IN at) (JJS least) (CD 85)) (NNS people)) (PP (IN in) (NP (NNP November))) (, ,) (PP (ADVP (NP (CD 29)) (RBR more)) (IN than) (IN in) (NP (NP (DT all)) (PP (IN of) (NP (DT the) (JJ previous) (NN winter)))))) (. .)))

%(ROOT (S (NP (NP (JJS Most)) (PP (IN of) (NP (DT the) (NNS victims)))) (VP (VBD were) (ADJP (JJ homeless) (SBAR (WHNP (NP (WP$ whose) (NNS deaths)) (PP (IN by) (NP (NN exposure)))) (S (VP (VBD were) (ADJP (RB alcohol) (VBN related))))))) (. .)))

%(ROOT (S (NP (NP (NNS Blizzards)) (CC and) (NP (JJ cold) (NNS temperatures))) (ADVP (RB also)) (VP (VBD hit) (NP (NNP Bulgaria) (CC and) (NNP Romania)) (, ,) (S (VP (VBG stranding) (NP (NNS hundreds)) (PP (IN in) (NP (PRP$ their) (NNS cars)))))) (. .)))

%(ROOT (S (ADVP (RB Elsewhere)) (, ,) (NP (NN snow)) (VP (VBD blanketed) (NP (NP (DT the) (JJ Italian) (NN island)) (PP (IN of) (NP (NNP Capri)))) (PP (IN for) (NP (NP (DT the) (JJ first) (NN time)) (PP (IN in) (NP (CD 10) (NNS years)))))) (. .)))
%

The corresponding entity grid for the text that is shown in Example~\ref{ex:rel-text} is presented in Table~\ref{tab:rel-egrid}. 
For constructing this grid, we follow \newcite{barzilay05a, barzilay08} and consider head nouns of noun phrases to represent the entities.  
The coreferent mentions are detected by string matching over head-nouns. 

\begin{table}[!ht]	
	\begin{center}
		\begin{tabular}{lcccccc}
			\toprule
			Entity  		& $s_0$ & $s_1$ & $s_2$ & $s_3$ & $s_4$ & $s_5$ 
			\\
			\midrule
			WAVE 			& S & - & - & - & - & - \\
			WORST 			& X & - & - & - & - & - \\
			YEARS 			& X & - & - & - & - & X \\
			PARTS 			& O & O & - & - & - & - \\
			EUROPE  		& X & - & - & - & - & - \\
			TEMPERATURES  	& O & - & X & - & S & - \\
			SCORES  		& O & - & - & - & - & - \\
			PEOPLE  		& X & - & O & - & - & - \\
			POLAND 			& - & O & X & - & - & - \\
			BULGARIA  		& - & X & - & - & - & - \\
			ROMANIA  		& - & X & - & - & O & - \\
			CENTRAL  		& - & X & - & - & - & - \\
			FRANCE  		& - & X & - & - & - & - \\
			NOVEMBER  		& - & - & X & - & - & - \\
			WEEKS 			& - & - & S & - & - & - \\
			ALL 			& - & - & X & - & - & - \\
			WINTER  		& - & - & X & - & - & - \\
			MOST 			& - & - & - & S & - & - \\
			VICTIMS  		& - & - & - & X & - & - \\
			DEATHS 			& - & - & - & X & - & - \\
			EXPOSURE  		& - & - & - & X & - & - \\
			BLIZZARDS  		& - & - & - & - & S & - \\
			HUNDREDS  		& - & - & - & - & O & - \\
			CARS  			& - & - & - & - & X & - \\
			TIME  			& - & - & - & - & - & X \\
			SNOW  			& - & - & - & - & - & S \\
			ISLAND 			& - & - & - & - & - & O \\
			CAPRI 			& - & - & - & - & - & X \\
			\bottomrule
		\end{tabular}
		\caption{
		The entity grid representation of the text presented in Example \ref{ex:rel-text}. The rows represent entities and columns encode sentences. 
        If an entity is mentioned in a sentence, their corresponding entry in the grid indicates the grammatical role of the mention in the sentence; otherwise the entry is marked by ``--''.
		} 
		\label{tab:rel-egrid}
	\end{center}
\end{table}

It is worth noting that although entities in the original version of the entity grid are indicated by the head of a noun phrase 
%(such as ``flight'' in Example \ref{ex:rel-text}), 
\newcite{elsner11a} show that adding non-head nouns 
%(like ``the personal \textbf{country} flight'' in Example \ref{ex:rel-text}) 
to a grid is beneficial to improve the representation power of the entity grid model. 
This enables the model to involve both head nouns and pre-modifiers in noun phrases to link sentences. 
Therefore, \newcite{elsner11a} consider all nouns 
%(both ``country'' and ``flight'' in the text shown in Example \ref{ex:rel-text}) 
as entities in the entity grid representation.  
The non-head mentions are given the role X. 

\subsection{Pattern Definition: Grammatical Transitions}

The key hypothesis in the entity grid model is that the way that entities are distributed as well as the way that the grammatical roles of entity mentions change through a text reveal similar patterns in coherent texts.  
\newcite{barzilay05a,barzilay08} predefine all possible transitions that may occur for an entity in a text as patterns. 

More concretely, they define a transition pattern as a sequence of employed symbols with size $n$, i.e., $\{ S,O,X,\textit{--} \}^n$. 
Each pattern represents entity occurrences between sentences and the way that their syntactic roles in $n$ adjacent sentences change. 
For instance, for two adjacent sentences ($n=2$) there are $16$ possible patterns.
These patterns are shown in Table~\ref{table:rel-egrid-pattern}. 

\begin{table}[!ht]
	\begin{center}
		\resizebox{\columnwidth}{!}
		{%
			\begin{tabular}{@{}cccccccccccccccc@{}}
			\toprule
			S S & S O & S X & S -- & 
			O S & O O & O X & O -- & 
			X S & X O & X X & X -- & 
			-- S & -- O & -- X & -- -- 
			\\
			\bottomrule	
			\end{tabular}
		}%
	\end{center}
	\caption{
	Patterns that are defined in the entity grid model.  
	Patterns represent all possible entity occurrences in two adjacent sentences. 
	Symbols S (subject), O (object), and X (other) show the grammatical role of an entity in a sentence. Symbol ``--'' encodes that an entity is not mentioned in a sentence.
	}
	\label{table:rel-egrid-pattern}
\end{table}

Each pattern that is shown in Table \ref{table:rel-egrid-pattern} represents one possible way in which an entity may occur in two adjacent sentences. 
For example, pattern ``S O'' encodes that an entity appears in two adjacent sentences, and its syntactic role is changing from subject to object across sentences. 
As another example, consider pattern ``S --''. 
It indicates that an entity is referred to by a mention in the subject position of a sentence, and the entity has no mention in the immediately next sentence. 

\subsubsection{Coherence Representation: Probabilities of Transitions}
%
The entity grid model revolves around the assumption that coherent texts reveal certain regularities over the frequency of transitions or patterns \cite{barzilay05a,barzilay08}.    
The frequency of patterns can be used as an indicator of the preference of coherent texts in using or avoiding certain transitions. 
However, in order to prevent the model to be biased towards the text length, the probability of each pattern, rather than its raw frequency, is computed. 	 
More formally, given the entity grid representation of a text, the probability of a transition pattern occurring in the grid is computed as follows:

\begin{equation}
p(t) = \frac{n(t)}{n(t^*)},
\end{equation}
where $t$ is a transition, $n(t)$ indicates the number of times that transition $t$ occurs in the entity grid, and denominator $n(t^*)$ depicts the number of occurrences of all patterns whose length is as same as the length of $t$ in the grid. 
For instance, consider the grid in Table~\ref{tab:rel-egrid}; the probability of pattern ``O O'' is $.01$, which is computed as a ratio of its frequency, i.e. one, divided by the total number of patterns of length two, i.e.,\ $140$. 
% \textbf{CHECK IF THIS GRID MATCHES WITH WHAT CAMILE GAVE YOU. IF NOT, YOU SHOULD SAY THAT THIS GRID, INCLUDING SYNTACTIC ROLES AND HEAD NOUN FINDERS, IS GENERATED BY BROWNCOHERENCE TOOLKIT. 
% HOW IS IT POSSIBLE THAT A SENTENCE DOES NOT HAVE ANY SUBJECT, SEE S2}   
Therefore, the coherence of a text can be represented by the distribution of patterns in the text. 
The entity grid model captures frequencies of entity transitions in the entity grid representation of a text with a vector, which represents the coherence of the text. 
This vector can be interpreted as a feature vector for the coherence of a text, where each feature is the probability of a pattern in the grid representation of the text.  
Table~\ref{tab:rel-egrid-probs} shows the feature vector representation of the grid presented in Table~\ref{tab:rel-egrid} using all transitions of length two. 

\begin{table}
	\begin{center}
		\resizebox{\columnwidth}{!}
		{%
			\begin{tabular}{@{}cccccccccccccccc@{}}
				\toprule
				S S & S O & S X & S -- & O S & O O & O X & O -- & X S & X O & X X & X -- & -- S & -- O & -- X & -- -- \\
				\midrule
				$.00$ & $.00$ & $.00$ & $.04$ & $.00$ & $.01$ & $.01$ & $.04$ & $.00$ & $.00$ & $.00$ & $.12$ & $.04$ & $.04$ & $.11$ & $.60$ \\
				\bottomrule
			\end{tabular}
		}%
	\end{center}
	\caption{
	Probabilities of the entity transition patterns that are introduced in Table~\ref{table:rel-egrid-pattern} in the entity grid shown in Table~\ref{tab:rel-egrid}.
	}
	\label{tab:rel-egrid-probs}
\end{table}

The centering theory and its extensions try to linguistically define and rank patterns such that they will be useful for assessing coherence and also solving a downstream task.  
The key advantage of the entity grid model is that it does not define any preference over transitions. 
It just computes probabilities of patterns in entity grids and defines them as features. 
The ranking or any other interplay among these features are learned by a machine learning model such as support vector machines. 

More concretely, given a dataset consisting of texts with different degrees of coherence, the entity grid model captures the coherence of each text in the dataset with a vector of transition probabilities. 
These vectors are supplied to machine learning models to distinguish texts concerning their coherence.  
Machine learning models automatically learn how coherence patterns should interact with each other to accomplish the final evaluation task.  
%We discuss more about evaluation tasks for coherence in Section \ref{sec:rel-coh-applications}.

\subsubsection{Extensions of the Entity Grid Model}
%
There are several extensions to the entity grid model in literature.  
They mostly extend the entity grid model in two ways. 
Some employ different approaches for entity identification, and some others use various linguistic information about entities to fill entires of grids. 

\newcite{filippova.enlg07} extend the entity grid approach by grouping all entities that are semantically related.  
They demonstrate that by grouping related entities, the performance of the entity grid model improves, especially when syntactic information is not involved. 
They use WikiRelate \cite{strube.aaai06} to compute relatedness between entities, $SemRel(e_i,e_j) >t$, where $t$ is a threshold.
Different values of $t$ result in different grid densities. 
For small values, a grid is almost dense since many entities are grouped into one.  

\newcite{elsner08b} employ the information status, i.e.\ new or the first mention vs given or subsequent mentions, of entities, rather than syntactic roles. 
They run a maximum-entropy classifier to assign each noun phrase (i.e.\ mention) a label $L_{np} \in \lbrace new, old \rbrace$. 
The coherence score of a text is then estimated by the product of probabilities over the information status of each mention. 
They show that adding such a classifier, which distinguishes discourse-new entities from discourse-old ones, improves the performance of the entity grid model, which uses grammatical role information.  
Another finding of their work is that incorporating pronouns in the entity definition phase of the model enhances the entity grid representation, and consequently, the performance of the coherence model. 
Indeed, pronoun resolution systems, as they are highly precise but specific coreference systems, can be used to acquire more meaningful references to entities. 

\newcite{elsner11b} extend the entity grid representation by distinguishing between important and unimportant entities. 
The motivation of this work is that the standard entity grid model uses no information about the entity itself in transitions; the probability of a transition is the same regardless of the entity that is under discussion. 
In order to involve information about entities, they associate each entity with some features, e.g.,\ Is\_Named\_entity, Has\_Singular\_Mention, Has\_Proper\_Mention, and the like. 
They show that by distinguishing salient entities from other ones, the discriminative performance of the entity grid model improves. 

\newcite{linziheng11a} use the grid representation, i.e.\ a two-dimensional matrix, but instead of modeling entity transitions, they model discourse relation transitions between sentences. 
The grid is filled in by discourse relations, which connect a term in a sentence with other sentences. 
Then, similar to the entity grid model the probabilities of transitions are used to represent the coherence of a text. 
In a follow-up paper, \newcite{fengvanessawei14} train the same model but using features derived from deep discourse structures annotated with Rhetorical Structure Theory (RST) relations \cite{mann88,prasad08a}. 
Early RST-based models include \newcite{marcu97b} and \newcite{mellish98}, which focus on coherent text generation rather than coherence evaluation. 

\newcite{nguyen17} propose a deep learning model to learn patterns in the entity grid representation of text. 
Their model first transforms grammatical roles in an entity grid into vector representations and then supplies them to a convolution operation to model entity transitions in a distributed space. 
The max-pooled features from the convoluted features are used for coherence scoring. 
In a later work, \newcite{joty18} extend their neural entity grid model by lexicalizing its entity transitions such that each entry of the entity grid contains two vectors, one representing its corresponding lexicon and one representing the grammatical role of the entity in the corresponding sentence.  

% \newcite{louis12} introduce a coherence model based on syntactic patterns in text by assuming that sentences in a coherent text should share the same structural syntactic patterns.
To conclude this part, we point out the advantages and disadvantages of the entity grid model. 
The main benefit of the entity grid model is that it learns the properties of coherent texts, which are represented by patterns of entity distributions, from a corpus of texts without recourse to manual annotations or a predefined knowledge base. 
However, the main limitation of the entity grid model is that it only takes into account relations between adjacent sentences, while in many cases adjacent sentences do not have any entities in common. 
For example, in an investigation on texts in the CoNLL 2012 dataset \cite{pradhan12}, it is shown that 42.34\% of adjacent sentences do not share any common entities \cite{zhangmuyu15}. 
Moreover, non-adjacent sentences can be related to each other as well. 
The entity grid model does not model such relation mainly because its grid representation cannot capture long-distance relations.  
It is worth noting that increasing the sequence length of grammatical transitions does not lead to incorporating long-distance relations between sentences.  
In practice, the length of sequences has never been fixed to a value higher than two. 
The reason is that enlarging the length of sequences increases the number of transitions, many of which do not frequently occur in texts. 
As a result, many transitions have zero probability in feature vector representations of the coherence of texts. 
This problem is known as ``sparsity'' in statistical machine learning. 
We discuss more about this problem in Chapter \ref{ch:lex-graph}. 

\subsection{The Entity Graph Model}
\label{sec:ent_graph}

The entity graph model \cite{guinaudeau13} represents entity-based relations between sentences in a text by a graph\footnote{We formally define the concepts in graphs that are required for the research in this thesis in Chapter \ref{ch:coh-patterns}}. 
Graphs are capable of spanning the entire text and capture connections between any two sentences in a text. 
Moreover, an advantage of formulating a problem like coherence modelings by graphs is that standard algorithms in graph theory can be employed to solve the problem. 
Thus, it is sufficient to encode a problem by graphs and then choose a proper solution from graph theory to solve the problem. 
Here, we review how \newcite{guinaudeau13} use a graph to represent the distribution of entities through a text, and how they use such representations to formulate and solve the task of coherence modeling by connectivity measures in graphs. 

\subsubsection{Text Representation: Entity Graphs}

The entity grid representations of texts are mostly sparse, which means many entries in grids are ``--'', because each sentence contains few entities, and the rest of entities are absent in the sentence. 
Graphs can deal with this sparsity issue in grid representations. 
\newcite{guinaudeau13} propose to recast the entity grid representation of a text by a graph representation. 
We refer to this representation as the entity graph because it captures the distribution of entities through a text via a graph. 
Figure~\ref{fig:rel-egraph} depicts the entity graph representation of the text in Example~\ref{ex:rel-text}, where the graph is constructed based on the entity grid shown in Table~\ref{tab:rel-egrid}. 

The idea is that the entity grid representation can be taken as the incidence matrix\footnote{The incidence matrix, or the adjacency matrix, of a graph is a two dimensional matrix A with binary elements. An entry is 1 if there is an edge between the nodes corresponding to the row and the column of the entry; otherwise, the value of the entry is 0.} 
of a bipartite graph, which consists of two disjoint sets of nodes.  
Node sets in the entity graph representation correspond with rows and columns in the entity grid representation.  
One set includes nodes associated with entities, and the other set includes nodes associated with sentences.  
Edges in the entity graph encode entries in the entity grid such that if a sentence contains a mention of an entity, then an edge connects the associated node with the sentence and the associated node with the entity in the entity graph. 
Therefore edges in the entity graph are equivalent to entries in the entity grid that are not equal to ``--''. 
The value of other entries in the entity grid are encoded as edge weights in the entity graph. 
More concretely, the grammatical role of an entity in a sentence is encoded in the entity graph by the weight of the edge that connects the entity node to the sentence node. 
Given the linguistic intuition that entities with important syntactic roles are prominent entities in each sentence, three numbers $3>2>1$ are used to model subject (S), object (O), and any others syntactic roles (X), which are employed by the entity grid model. 
% Entity graph model is established on the entity grid representation of a text. 
% Therefore, the way of obtaining entities in the entity grid representation affects the performance of the entity grid model as well.  
% Similar to \newcite{elsner11b}, \newcite{guinaudeau13} take all nouns in a text as mentions of entities, even those that are not head of any noun phrase. 

\begin{figure}[!ht]
	\begin{center}
	\resizebox{\columnwidth}{!}
	{%
	\begin{tabular}{@{}c@{}}
		\begin{tikzpicture}[shorten >=1pt,-,scale=0.5]  
				
				\tikzstyle{sentence}=[circle,thick,draw=black!90,fill=black!10,minimum size=2mm]
				\tikzstyle{entity}=[circle,thick,draw=black!90,fill=black!10,minimum size=5mm]
				\tikzstyle{edge}=[draw=black!90, thick]
			   
			   \begin{scope}
			   
				 \node [sentence] (s0) at (-3,0) {\small{$s_0$}};
				 \node [sentence] (s1) at (2,0) {\small{$s_1$}};
				 \node [sentence] (s2) at (7,0) {\small{$s_2$}}; 
				 \node [sentence] (s3) at (12,0) {\small{$s_3$}}; 
				 \node [sentence] (s4) at (17,0) {\small{$s_4$}};
				 \node [sentence] (s5) at (22,0) {\small{$s_5$}}; 

				 \node [entity, label=below:\rotatebox{+90}{\tiny{WAVE}}] (e0)  at (-6.0,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{WORST}}] (e1)  at (-4.9,-7) {};
				 \node [entity, label=below:\rotatebox{+90}{\tiny{YEARS}}] (e2)  at (-3.8,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{PARTS}}] (e3)  at (-2.7,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{EUROPE}}] (e4)  at (-1.6,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{TEMPRATURES}}] (e5)  at (-0.5,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{SCORES}}] (e6)  at (0.6,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{PEOPLE}}] (e7)  at (1.7,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{POLAND}}] (e8)  at (2.8,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{BULGARIA}}] (e9)  at (3.9,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{ROMANIA}}] (e10)  at (5.0,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{CENTERAL}}] (e11)  at (6.1,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{FRANCE}}] (e12)  at (7.2,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{NOVEMBER}}] (e13)  at (8.3,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{WEEKS}}] (e14)  at (9.4,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{ALL}}] (e15)  at (10.5,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{WINTER}}] (e16)  at (11.6,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{MOST}}] (e17)  at (12.7,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{VICTIMS}}] (e18)  at (13.8,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{DEATHS}}] (e19)  at (14.9,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{EXPOSURE}}] (e20)  at (16.0,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{ALCOHOL}}] (e21)  at (17.1,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{BILIZZARDS}}] (e22)  at (18.2,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{HUNDREDS}}] (e23)  at (19.3,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{CARS}}] (e24)  at (20.4,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{TIME}}] (e25)  at (21.5,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{SNOW}}] (e26)  at (22.6,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{ISLAND}}] (e27)  at (23.7,-7) {}; 
				 \node [entity, label=below:\rotatebox{+90}{\tiny{CARPI}}] (e28)  at (24.8,-7) {}; 

				 
				 \path[edge] (s0) edge [above, very near end] node[font=\tiny] {$3$} (e0.north); %, line width=0.3ex
				 \path[edge] (s0) edge [above, very near end] node[font=\tiny] {$1$} (e1.north);
				 \path[edge] (s0) edge [above, very near end] node[font=\tiny, xshift=-1mm] {$1$} (e2.north);
				 \path[edge] (s0) edge [above, very near end] node[font=\tiny, xshift=-1mm] {$2$} (e3.north);
				 \path[edge] (s0) edge [above, very near end] node[font=\tiny, yshift=3mm, xshift=-1.5mm] {$1$} (e4.north);
				 \path[edge] (s0) edge [above, very near end] node[font=\tiny, xshift=-2mm] {$2$} (e5.north);
				 \path[edge] (s0) edge [above, very near end] node[font=\tiny, xshift=-2mm] {$2$} (e6.north);
				 \path[edge] (s0) edge [above, very near end] node[font=\tiny, yshift=2mm, xshift=-3.7mm] {$1$} (e7.north);

				 \path[edge] (s1) edge [above, near start] node[font=\tiny, near start] {$3$} (e3.north);
				 \path[edge] (s1) edge [above, near start] node[font=\tiny, xshift=-1.5mm] {$3$} (e8.north);
				 \path[edge] (s1) edge [above, near start] node[font=\tiny, xshift=-1mm] {$3$} (e9.north);
				 \path[edge] (s1) edge [above, very near end] node[font=\tiny, xshift=-2mm] {$3$} (e10.north);
				 \path[edge] (s1) edge [above, very near end] node[font=\tiny, xshift=-2mm] {$1$} (e11.north);
				 \path[edge] (s1) edge [above, very near start] node[font=\tiny] {$1$} (e12.north);

				 \path[edge] (s2) edge [above, very near end] node[font=\tiny, yshift=4.4mm, xshift=6.5mm] {$1$} (e5.north);
				 \path[edge] (s2) edge [above, near start] node[font=\tiny, xshift=1mm] {$2$} (e7.north);
				 \path[edge] (s2) edge [below, near start] node[font=\tiny] {$1$} (e8.north);
				 \path[edge] (s2) edge [below,  near end] node[font=\tiny] {$1$} (e13.north);
				 \path[edge] (s2) edge [above, very near end] node[font=\tiny] {$3$} (e14.north);
				 \path[edge] (s2) edge [above, very near end] node[font=\tiny] {$1$} (e15.north);
				 \path[edge] (s2) edge [above, very near end] node[font=\tiny] {$1$} (e16.north);

				 \path[edge] (s3) edge [below,  near end] node[font=\tiny,xshift=-1mm] {$3$} (e17.north);
				 \path[edge] (s3) edge [below,  near end] node[font=\tiny] {$1$} (e18.north);
				 \path[edge] (s3) edge [below,  near end] node[font=\tiny] {$1$} (e19.north);
				 \path[edge] (s3) edge [below,  near end] node[font=\tiny] {$1$} (e20.north);
				 \path[edge] (s3) edge [below,  near end] node[font=\tiny] {$1$} (e21.north);

				 \path[edge] (s4) edge [above, very near start] node[font=\tiny] {$3$} (e5.north);
				 \path[edge] (s4) edge [below, very near start] node[font=\tiny] {$1$} (e9.north);
				 \path[edge] (s4) edge [above, midway] node[font=\tiny] {$2$} (e10.north);
				 \path[edge] (s4) edge [above, very near end] node[font=\tiny] {$1$} (e22.north); 
				 \path[edge] (s4) edge [above, very near end] node[font=\tiny] {$2$} (e23.north); 
				 \path[edge] (s4) edge [above, very near end] node[font=\tiny] {$1$} (e24.north);     

				 \path[edge] (s5) edge [above, very near start] node[font=\tiny, xshift=4mm] {$1$} (e2.north);
				 \path[edge] (s5) edge [above, midway] node[font=\tiny,xshift=-1mm] {$1$} (e25.north);
				 \path[edge] (s5) edge [above, midway] node[font=\tiny,xshift=-1mm] {$3$} (e26.north);
				 \path[edge] (s5) edge [above, midway] node[font=\tiny] {$2$} (e27.north); 
				 \path[edge] (s5) edge [above, midway] node[font=\tiny] {$1$} (e28.north);   

				\end{scope}        
			  \end{tikzpicture}
		\end{tabular}
		}%
	\end{center}
	\caption{
	The entity graph representation of the text presented in Example \ref{ex:rel-text}. 
	The graph is obtained from the entity grid representation shown in Table \ref{tab:rel-egrid}. 
	The top nodes represent columns in the grid or sentences in the text. 
	The bottom nodes capture rows in the grid or entities in the text. 
	Edges encode the entries in the grid. 
	Weights of edges represent the value of each entry in the grid: 3:S, 2:O, 1:X, and 0:--. 
	The weight of 0 is equivalent to no edge in a graph, so they are not drawn in the graph.  
	}
	\label{fig:rel-egraph}
\end{figure}

\subsubsection{Coherence Measurement: The Average Outdegree of Projection Graphs}

\paragraph{Projection graphs.}
Local coherence is about the connectivity among sentences in a text. 
However, nodes in the entity graphs consist of two disjoint node sets, one of which represents sentences. The other set captures entities. 
\newcite{guinaudeau13} propose to transform an entity graph to a graph whose nodes capture only sentences and whose edges encode entity-based relations among sentences. 
Such a graph, which is obtained from the entity graph as a bipartite graph, is called a ``one-mode projection graph'' (or a projection graph for the sake of brevity) in graph theory \cite{newmanmark10}. 
Edges in projection graphs are weighted in different ways in order to retain specific information about relations between sentence nodes and entity nodes in entity graphs. 
Moreover, edges in projection graphs are directed to encode the order of sentences in a text. 

\newcite{guinaudeau13} apply three kinds of projections, namely $P_U$, $P_W$ and $P_{Acc}$. 
Figure \ref{fig:rel-proj} shows these graphs obtained from the entity graph presented in Figure \ref{fig:rel-egraph}. 
These projection graphs differ in the weighting scheme that they use: 

\begin{itemize}

	\item In $P_U$, weights are binary, i.e., 0 or 1. 
	The weight of an edge between two nodes in this type of the projection graph is equal to $1$ if the corresponding sentence nodes are connected to at least one entity node in the entity graph.  
	This projection graph merely captures which sentences are linked to each other in a text. 

	\item In $P_W$, an edge is weighted according to the number of the entity nodes that are connected with both sentence nodes in the entity graph. 
    In other words, the weight of an edge between two nodes in this type of the projection graph represents the number of shared entities by the corresponding sentences. 
    This projection graph not only models that which sentences in a text are connected,  but also captures how strongly they are connected. 
    It takes the number of common entities between a pair of sentences as the strength of the relation between sentences. 

	\item In $P_{Acc}$, syntactic information is accounted for by integrating the edge weights in the entity graph. 
	In this case, the weight of the edge between nodes $s_i$ and $s_k$ is equal to

	\begin{equation}
		W_{ik} = \sum_{e \in E_{ik}}{w(e,s_i) \cdot w(e,s_k)},
	\end{equation}
	%
	where $E_{ik}$ is the set of the entity nodes that are connected to both $s_i$ and $s_k$ in the entity graph.   
	This type of the projection graph incorporates grammatical information about entities shared by sentences in order to measure the strength of the relation between sentences. 

\end{itemize}


\begin{figure}[!ht]
	\begin{center}
		\begin{tabular}{@{}lc@{}}
			\begin{tikzpicture}        
					 \node [] (n0) at (0,0) {};
			         \node [] (label) at (0,0.8) {$P_U:$};
			\end{tikzpicture}
			&
			\begin{tikzpicture}[shorten >=1pt,->,scale=0.5]  
        		\tikzstyle{sentence}=[circle,thick,draw=black!90,fill=black!10,minimum size=2mm]
				\tikzstyle{entity}=[circle,thick,draw=black!90,fill=black!10,minimum size=2mm]
        		\tikzstyle{edge}=[draw=black!90, thick]

       			\begin{scope}

			         \node [sentence] (s0) at (0,0) {\tiny{$s_0$}};
			         \node [sentence] (s1) at (3,0) {\tiny{$s_1$}};
			         \node [sentence] (s2) at (6,0) {\tiny{$s_2$}}; 
			         \node [sentence] (s3) at (9,0) {\tiny{$s_3$}}; 
			         \node [sentence] (s4) at (12,0) {\tiny{$s_4$}};
			         \node [sentence] (s5) at (15,0) {\tiny{$s_5$}}; 
 
			 		\path[edge] (s0) edge [above] node[font=\tiny]{} (s1);
			 		\path[edge, bend left = 30] (s0) edge [above] node[font=\tiny]{$1$} (s2);
					\path[edge, bend left = 30] (s0) edge [above] node[font=\tiny]{$1$} (s4);
			 		\path[edge, bend left = 30] (s0) edge [above] node[font=\tiny]{$1$} (s5);

			 		\path[edge] (s1) edge [above] node[font=\tiny]{$1$} (s2);
					\path[edge, bend right = 30] (s1) edge [above] node[font=\tiny]{$1$} (s4);
           
		        \end{scope}        
     		 \end{tikzpicture}

			 \\

			\begin{tikzpicture}        
				 \node [] (n0) at (0,0) {};
        		 \node [] (label) at (0,0.8) {$P_W:$};
			\end{tikzpicture}
 			&
			\begin{tikzpicture}[shorten >=1pt,->,scale=0.5]  
        			\tikzstyle{sentence}=[circle,thick,draw=black!90,fill=black!10,minimum size=2mm]
					\tikzstyle{entity}=[circle,thick,draw=black!90,fill=black!10,minimum size=2mm]
			        \tikzstyle{edge}=[draw=black!90, thick]
 			 
 			       \begin{scope}
       
     				    \node [sentence] (s0) at (0,0) {\tiny{$s_0$}};
				         \node [sentence] (s1) at (3,0) {\tiny{$s_1$}};
				         \node [sentence] (s2) at (6,0) {\tiny{$s_2$}}; 
				         \node [sentence] (s3) at (9,0) {\tiny{$s_3$}}; 
				         \node [sentence] (s4) at (12,0) {\tiny{$s_4$}};
				         \node [sentence] (s5) at (15,0) {\tiny{$s_5$}}; 
				 
				 		\path[edge                ] (s0) edge [above, midway] node[font=\tiny]{$1$} (s1);
				 		\path[edge, bend left = 30] (s0) edge [above, near end] node[font=\tiny]{$2$} (s2);
						\path[edge, bend left = 30] (s0) edge [above, near end] node[font=\tiny]{$1$} (s4);
				 		\path[edge, bend left = 30] (s0) edge [above, midway] node[font=\tiny]{$1$} (s5);

				 		\path[edge                 ] (s1) edge [above, midway] node[font=\tiny]{$1$} (s2);
						\path[edge, bend right = 30] (s1) edge [above, midway] node[font=\tiny]{$2$} (s4);
           
    			    \end{scope}        
   		   \end{tikzpicture}

			\\

			\begin{tikzpicture}        
				 \node [] (n0) at (0,0) {};
		         \node [] (label) at (0,0.8) {$P_{Acc}:$};
			\end{tikzpicture}
 			&

		   \begin{tikzpicture}[shorten >=1pt,->,scale=0.5]  
		        \tikzstyle{sentence}=[circle,thick,draw=black!90,fill=black!10,minimum size=2mm]
				\tikzstyle{entity}=[circle,thick,draw=black!90,fill=black!10,minimum size=2mm]
		        \tikzstyle{edge}=[draw=black!90, thick]

		        \begin{scope}

			         \node [sentence] (s0) at (0,0) {\tiny{$s_0$}};
			         \node [sentence] (s1) at (3,0) {\tiny{$s_1$}};
			         \node [sentence] (s2) at (6,0) {\tiny{$s_2$}}; 
			         \node [sentence] (s3) at (9,0) {\tiny{$s_3$}}; 
			         \node [sentence] (s4) at (12,0) {\tiny{$s_4$}};
			         \node [sentence] (s5) at (15,0) {\tiny{$s_5$}}; 
 
			 		\path[edge                ] (s0) edge [above, midway] node[font=\tiny]{$6$} (s1);
			 		\path[edge, bend left = 30] (s0) edge [above, near end] node[font=\tiny]{$4$} (s2);
					\path[edge, bend left = 30] (s0) edge [above, near end] node[font=\tiny]{$6$} (s4);
			 		\path[edge, bend left = 30] (s0) edge [above, midway] node[font=\tiny]{$1$} (s5);

			 		\path[edge                 ] (s1) edge [above, midway] node[font=\tiny]{$3$} (s2);
					\path[edge, bend right = 30] (s1) edge [above, midway] node[font=\tiny]{$9$} (s4);
           
		        \end{scope}        
      
      		\end{tikzpicture}

		\end{tabular}
	\end{center}
	\caption{
	Three types of projection graphs that are employed by the entity graph model. 
	$P_U$ shows only which sentence nodes are connected. 
	$P_W$ takes the number of shared entities as the weight of edges. 
	$P_{Acc}$ involves grammatical roles of common entities between sentences. 
	}
	\label{fig:rel-proj}
\end{figure}

Distances between sentences can be integrated into the weighting scheme of edges in projection graphs to decrease the importance of links between non-adjacent sentences \cite{guinaudeau13}.   
In this case, edge weights in projection graphs are divided by the difference between sentence IDs. 
%, i.e.\ the number of sentences in between of two sentences plus one.  

\paragraph{The average outdegree as a coherence metric.}
Given a projection graph representation of a text the coherence of the text is measured based on the connectivity of nodes in the projection graph. 
\newcite{guinaudeau13} define a coherence metric given the assumption that projection graphs of coherent texts contain more edges than projection graphs of incoherent ones.  
They propose to use a centrality metric \cite{newmanmark10} from the graph theory to measure to what extent nodes in a graph are connected with each other. 
Let $outDegree(s)$ be the sum of the weights associated to edges that leave node $s$ in projection graph $P$, then the centrality metric of the projection graph is computed by the average outdegree of all nodes ($N$) in the graph: 

\begin{equation}
	 AvgOutDeg(P) = \frac{1}{N} \sum_{i=1}^{N} outDegree(s_i).
\end{equation}

Table \ref{tab:rel-od} shows the AvgOutDeg for different projection graphs presented in Figure \ref{fig:rel-proj} with and without incorporating the distance information. 

\begin{table}[!ht]
	\begin{center}
		\begin{tabular}{ll}
			\hline
			 $P$ & $ AvgOutDeg(P)$ \\\hline
			 $P_U$ & $\frac{1}{6} \left((1+1+1+1)+(1+1)+(0)+(0)+(0)+(0)) \right) = 1.00$ \\
			 $P_W$ & $\frac{1}{6} \left((1+2+1+1)+(1+2)+(0)+(0)+(0)+(0)) \right) = 1.33$\\
			 $P_{Acc}$ &$\frac{1}{6} \left((6+4+6+1)+(3+9)+(0)+(0)+(0)+(0)) \right) = 4.83$ \\
			 $P_U\textit{, }Dist$ & $\frac{1}{6} \left((1+0.50+0.25+0.20)+(1+0.33)+(0)+(0)+(0)+(0)) \right) = 0.55$ \\
			 $P_W\textit{, }Dist$ & $\frac{1}{6} \left((1+1+0.25+0.20)+(1+0.66)+(0)+(0)+(0)+(0)) \right)= 0.69$ \\
			 $P_{Acc}\textit{, }Dist$ & $\frac{1}{6} \left((6+2+1.5+0.2)+(3+3)+(0)+(0)+(0)+(0)) \right)= 2.61$ \\
			 \hline
		\end{tabular}
	\end{center}
	\caption{The average outdegree of nodes in projection graphs presented in Figure \ref{fig:rel-proj}
	. Dist.\ shows when distance is integrated in edge weights. }
	\label{tab:rel-od}
\end{table}

In order to rank a pair of texts with respect to their coherence, \newcite{guinaudeau13} represent both texts with the same type of projection graphs, and then use the average outdegree of their projection graphs to compare texts. 
It is worth to mention that the described entity graph model is an unsupervised model as average outdegrees are directly employed for comparing texts with respect to their coherence. 
However, the average outdegree captures no information about the connectivity style of nodes in a projection graph. 
For example, consider two projection graphs that are shown in Figure \ref{fig:rel-avgod-weakness}. 
These two graphs have the same average outdegree, i.e.\ five, but graph (b) is disconnected because node $s_2$ is connected to none of its previous nodes. 
Consequently, its corresponding text is less coherent than the text associated with (a).
The outdegree does not capture such information. 

\begin{figure}[!ht]
	\begin{center}
		\begin{tabular}{@{}c@{}}
			\begin{tikzpicture}[shorten >=1pt,->,scale=0.5]  
        		\tikzstyle{sentence}=[circle,thick,draw=black!90,fill=black!10,minimum size=2mm]
				\tikzstyle{entity}=[circle,thick,draw=black!90,fill=black!10,minimum size=2mm]
        		\tikzstyle{edge}=[draw=black!90, thick]

       			\begin{scope}

			         \node [sentence] (s0) at (0,0) {\tiny{$s_0$}};
			         \node [sentence] (s1) at (3,0) {\tiny{$s_1$}};
			         \node [sentence] (s2) at (6,0) {\tiny{$s_2$}}; 
			         \node [sentence] (s3) at (9,0) {\tiny{$s_3$}}; 
			         \node [sentence] (s4) at (12,0) {\tiny{$s_4$}};
			         \node [sentence] (s5) at (15,0) {\tiny{$s_5$}}; 
 
			 		\path[edge] (s0) edge (s1);
			 		\path[edge] (s1) edge (s2);
			 		\path[edge] (s2) edge (s3);
			 		\path[edge] (s3) edge (s4);
			 		\path[edge] (s4) edge (s5);
			 		
		        \end{scope}        
     		 \end{tikzpicture}
     		\\
     			(a) 
			\\
			\begin{tikzpicture}[shorten >=1pt,->,scale=0.5]  
        			\tikzstyle{sentence}=[circle,thick,draw=black!90,fill=black!10,minimum size=2mm]
					\tikzstyle{entity}=[circle,thick,draw=black!90,fill=black!10,minimum size=2mm]
			        \tikzstyle{edge}=[draw=black!90, thick]
                  \begin{scope}

			         \node [sentence] (s0) at (0,0) {\tiny{$s_0$}};
			         \node [sentence] (s1) at (3,0) {\tiny{$s_1$}};
			         \node [sentence] (s2) at (6,0) {\tiny{$s_2$}}; 
			         \node [sentence] (s3) at (9,0) {\tiny{$s_3$}}; 
			         \node [sentence] (s4) at (12,0) {\tiny{$s_4$}};
			         \node [sentence] (s5) at (15,0) {\tiny{$s_5$}}; 
 
			 		\path[edge] (s0) edge (s1);
			 		
			 		\path[edge] (s2) edge (s3);
			 		\path[edge] (s3) edge (s4);
			 		\path[edge] (s4) edge (s5);
			 		\path[edge, bend left = 30] (s2) edge (s5);
                  \end{scope}
   		   \end{tikzpicture}
   		   \\
   		   (b)
   		    
		\end{tabular}
	\end{center}
	\caption{(a): A projection graph with the outdegree of five, and all nodes are in one component; (b): A projection graph with the outdegree of five with two components.}
	\label{fig:rel-avgod-weakness}
\end{figure}

Moreover, three types of projection graphs behave differently for different tasks examined by \newcite{guinaudeau13}. 
So it is not clear which type of projection graphs is more useful for a downstream task. 

\subsubsection{Extensions of the Entity Graph Model}

The entity graph model is extended from two different perspectives: text representation and the graph metric that is employed to measure coherence. 

\newcite{dias15} propose to fill in the grid based on the RST relations between sentences in a text. 
An entry in the entity grid is 1 if an entity is part of a sentence that participates in an RST relation.
Based on such grid representations, they define a bipartite graph similar to the entity graph and then construct its projection graph to model relations among sentences. 
Then, they use the average outdegree of projection graphs to measure the coherence of a text. 
Their model outperforms the entity graph model. 
However, similar to RST-based extensions of the entity grid model, obtaining RST relations is subjective, and human annotations or discourse parsers are not available for many languages. 

\newcite{petersen15} use several graph metrics, rather than the average outdegree, to approximate different aspects of the text flow that can indicate coherence.  
These metrics are designed to capture more information about the connectivity style of nodes in projection graphs. 
For example, they leverage the mean of the PageRank scores \cite{newmanmark10} of nodes in a projection graph for distinguishing a star-graph, in which all nodes are connected to one node, and no other edges occur, and a path graph, in which all nodes occur in a chain. 
Some other assessed metrics include a clustering coefficient, which measures to what extent neighbors of a node are connected among themselves; and betweenness, which is the fraction of shortest paths that contain a node. 
Although their results are better than the original entity graph which uses the average outdegree, the difference is not substantial to consider these new metrics. 
We admit that the average outdegree is quite straightforward and efficient to compute. 

\newcite{zhangmuyu15} use semantic relations between entities to identify not only the mentions that refer to the same entity but also the mentions that refer to entities which are semantically related, e.g., \ ``Gates'' and ``Microsoft''.  
They capture such semantic relations by leveraging WordNet \cite{baccianella10} as a knowledge base, where the semantic relation between entities is limited to argument$_1$-predicate-argument$_2$ patterns, such as \mbox{Gates-create-Microsoft}. 
By incorporating such relations, the performance of the entity graph model improves.  
They also challenge the average outdegree metric and propose to combine this metric with another score which is named ``reachability''. 
The reachability score is the sum of the weights of edges in the shortest path that starts from the first sentence node and ends at the current sentence node. 
The intuition behind the reachability score is that this score reflects the tightness between a sentence in a text and its previous context in the text. 
In this way, they overcome some weaknesses of the average outdegree but not all of them.    

We propose\footnote{We just briefly explain this model here because it does not focus on coherence patterns which are the core of the research presented in this thesis.} an extension of the entity graph model by taking the entity and sentence importance into account \cite{mesgar14}. 
We reflect the connectivity structure of an entity graph into its edge weights by applying a normalization method to the weights.  
The normalization method reduces the differences in performance of three types of projection graphs. 

\section{Lexical Approaches to Local Coherence}

Local coherence is an essential factor in text comprehension.
It is about the extent to which sentences in a text are linked together. 
\newcite{halliday76} emphasize the role of lexical cohesion in connecting sentences in a text to each other. 
They consider several linguistic devices -- repetition, synonymy, hyponymy, and meronymy --  which contribute to the ``continuity of lexical meaning'' observed in a coherent text.   
In this section, we mainly survey the computational models that use lexical relations for modeling local coherence. 
However, since these models are based on the lexico-semantic relations between words in a text, we first discuss different resources that are used in the literature to recognize such relations. 

\subsection{Lexical Resources} 

Most lexical models, including coherence models, crucially rely on the existence of resources that encode information about semantic relations between words in a language. 
Such resources are typically acquired via two main approaches: the knowledge-based approach, or top-down, where humans manually curate such information, and the corpus-based approach, or bottomup, where information is automatically learned from corpora. 
Although the latter has gained ground during the last decades due to the availability of large amounts of texts and increased computing capacities, the former remains fundamental because it allows us to collect reliable, fine-grained, and explicit information. 

\paragraph{Knowledge-based resources.}
One of the fundamental lexical knowledge resources for English is the Princeton WordNet \cite{fellbaum98}.  
WordNet aims to represent real-world concepts and their relations similar to what humans perceive about them. 
WordNet covers about 20 million instances of concepts and relations extracted from raw texts. 
Nouns, verbs, adjectives, and adverbs are each organized into networks of synonym sets, which is called ``synsets''. 
Each synset represents one lexical concept and a variety of its relations. 
The WordNet-based similarity measures have been shown to correlate with human similarity judgments reliably. 
WordNet has been used in a variety of applications, ranging from malapropism detection to word sense disambiguation \cite{budanitsky06}.  
The Princeton WordNet for English inspired the creation of a lexical knowledge base in some other languages such as German, which is called GermaNet \cite{hamp97}. 
However, WordNet is not available for many languages because it requires a lot of human effort and knowledge for annotation. 

YAGO \cite{hoffart13} is another example of top-down knowledge resources.  
It consists of four million instances of concepts and relations where the instances are automatically extracted from online encyclopedias such as Wikipedia \cite{denoyer06} and FreeBase \cite{bollacker08}. 
Relations then are edited by humans. 
Generally speaking, manually defined knowledge bases, like WordNet, have better accuracy but lower coverage, while automatically extracted knowledge bases, like YAGO, are the opposite. 

Although different coherence models have employed these knowledge bases, there are weaknesses with these resources.   
\newcite{zhangmuyu15} describe two main issues in retrieving knowledge about words from such resources as follows: 

\begin{itemize}
\item knowledge source: Which resource is the best for obtaining this knowledge? 
\item knowledge selection: How do we pinpoint the most relevant entry in a knowledge base?
\end{itemize}

Knowledge resources cover semantic relations between particular sets of word categories.  
For example, WordNet is designed to provide complete coverage of common open-class English words. 
Therefore, it has little or no coverage of vocabulary from specialized domains and has minimal coverage of proper nouns. 
This issue may hinder its application to domain-specific contexts and tasks which require to deal with proper nouns. 
The issue related to knowledge selection refers to how we should retrieve knowledge instances. Should we use exact or partial matching of words?   
The chance of exact matching of words (especially entities) in a text with instances in a knowledge base is low.  
In contrast, partial matching between arguments and entities usually increases coverage but at the risk of introducing some noise. 

Finally, regardless of which knowledge resource is employed, a similarity metric is required to quantify if two words are semantically related or not.  
For these knowledge resources, a simple way to compute the semantic relations between words is to view the knowledge base as a graph. 
The semantic relatedness can be measured based on graph properties such as the path length between the words \cite{budanitsky06}. 
The shorter the path between two word nodes, the more similar the words are. 

\paragraph{Corpus-based resources.} 
The bottom-up knowledge resources are obtained based on the co-occurrence of words in texts.  
In these resources, words are taken similar if they frequently occur in a similar context. 
The main idea of resources is to represent semantic properties of words by vectors in a \mbox{multi-dimensional} space. 
Such vectors are obtained by observing the distributional patterns of word co-occurrences with their nearby words in large bodies of texts. 
The similarity between word vectors in vector space quantifies the semantic relatedness between words in language space. 

Different approaches to learning such vector representations exist.  
One of the early techniques is ``Latent Semantic Analysis'' (LSA) proposed by \newcite{landauer97}. 
This method constructs a matrix, namely a co-occurrence matrix, containing word counts per text from a large number of texts.   
It then uses a mathematical method called ``Singular Value Decomposition'' (SVD) \cite{furnas88} to reduce the text dimensionality in the co-occurrence matrix while preserving the similarity structure among words. 

The other method is named ``Latent Dirichlet Allocation'' (LDA) proposed by \newcite{blei03}. 
LDA is a generative statistical model which allows sets of words to be explained by a set of latent topics.  
Its intuition is that words of text with similar topics are semantically related. 
It is worth noting that in this approach, topics are neither semantically nor epistemologically defined. 
Topics are defined on the basis of automatic detection of the likelihood of term co-occurrence. 
This method is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior.
The intuition of sparse Dirichlet priors is that texts cover a small set of topics and those topics are frequently expressed by means of a small set of words. 
In practice, LDA yields to better disambiguation of words and more precise assignments of texts to topics in comparison to LSA. 
%A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.

Finally, recent methods for representing words in a distributional space utilize deep neural networks rather than co-occurrence matrices. 
In contrast to the above methods (LSA and LDA) which are unsupervised, the neural network models applied for obtaining word vectors are trained a supervised manner.  This property is an advantage for these methods because as the vocabulary in a language grows, new vectors for new vocabulary can be trained and appended to such knowledge resources.  
The word vectors that are generated by these models are named ``word embeddings''. 
Well-known approaches for obtaining word embedding are ``word2vec''  \cite{mikolov13} and ``GloVe'' \cite{pennington14}. 
The word2vec approach focuses on learning the embeddings of a word given its local usage context, where a window of words surrounding the word defines its context. 
The length of the window is a configurable parameter of the model. 
Large windows tend to produce more topical similarities, and smaller windows give more functional and syntactic similarities \cite{goldberg17}. 
The GloVe approach, rather than using a small window to define local context,  constructs an explicit word-context or word co-occurrence matrix using statistics across the whole text. 

Indeed, distributional representations of words, in general, are beneficial if they are trained on sufficiently large and balanced corpora; otherwise, there is a risk of finding words whose similarity only makes sense in the examined corpus \cite{lindekang98b}.  
See \newcite{budanitsky06} where they highlight several problems that arise from the imbalance and sparseness of corpora for such methods. 

In resources that are provided bottom-up, the cosine of the angle of a pair of word embeddings (or the inner product between the normalizations of the two vectors) measures the similarity of the two words. 
Cosine values near one represent semantically similar words, while values near zero represent independent words and values near -1 represent semantically different words. 

\subsection{Lexical Cohesion Models}

Here, we review local coherence models that are upon lexical cohesion \cite{halliday76}, i.e., lexical relations between words. 
We categorize these approaches into three high-level research trends: models that are based on lexical chains,  models that are based on sentence similarities, and models that are based on word distributions.  

The first trend of research includes methods that use lexical relations to build lexical chains. 
A lexical chain is a sequence of semantically related words spanning a topical text unit \cite{morris91}. 
Lexical chaining has a long history in local coherence modeling for different applications \cite{morris91,fenglijun09,wongbillytm12,benguosheng13,flor13}. 
\newcite{morris91} induce semantic relations from Roget's Thesaurus as a knowledge resource. 
The thesaurus provides an account of the vocabulary of English, grouped into hierarchical categories. 
Their central intuition is that coherent texts have a high concentration of dense chains. 
Therefore, the distribution of lexical chains is a surface indicator of the structure of coherent texts. 
\newcite{galley03a} construct lexical chains for topic segmentation, which is tightly related to coherence. 
This model does not need any knowledge resource because it builds lexical chains based merely on word repetitions. 
In contrast, \newcite{stokes04} employ WordNet to extract lexical chains from texts.  
Weak\footnote{The difference between weak and strong relations is identified using a threshold on the employed similarity function.} relations between words in lexical chains in a text are used as an indicator of topic shifts in texts. 
\newcite{barzilay97} propose a lexical chaining algorithm which uses WordNet, Thesaurus, and Part of Speech (POS) tags to extract lexical relations. 
They do not use this model directly for coherence modeling, but they utilize it for generating coherent summaries. 
\newcite{somasundaran14} use lexical chaining for measuring the coherence of essays written by non-native English students. 
To do so, they employ Lins thesaurus \cite{lindekang98} to identify semantically similar words in essays.   
The main lesson learned from this work is that features related to lexical chains measure the coherence of essays. 
In order to capture different aspects of a lexical chain, they employ several features related to lexical chains, such as the number of chains in a text, the size of a chain, and the number of chains with more than one word. 

The second trend of research related to lexical cohesion includes papers that consider lexical relations between words of sentences in order to compute the similarity between sentences.    
The central insight of these approaches, in general, is that sentences of coherent texts are similar because they contain semantically related words. 
Generally, these models first aggregate word vectors corresponding to words in sentences for representing each sentence via a vector, and then computes the similarity between sentence vectors to measure the similarity between two sentences.  
Finally, the average of all similarities between adjacent sentences in a text measures the coherence of the text: 

\begin{equation}
coh(T) = \frac{\sum_{i=0}^{N-2}sim(s_i,s_{i+1})}{N-1},
\end{equation} 

where $sim(s_i, s_{i+1})$ is a measure of similarity between two adjacent sentences. 
This idea is operationalized in different ways.
For example, \newcite{foltz98} employ LSA to represent each word in a sentence by a vector and then use the weighted average of word vectors to obtain sentence vectors. 
The weighting scheme in their approach is inspired by information retrieval techniques, most notably TF-IDF, where TF stands for the ``Term Frequency'' and IDF for the ``Inverse Document Frequency''.
The similarity between two adjacent sentences is computed by applying the cosine function to sentence vectors. 
\newcite{higgins07} apply a similar strategy for essay coherence, but they use a Random Indexing (RI) model to represent sentences. 
\newcite{lapata05a} compute the similarity between two adjacent sentences by counting the number of exact repetitions between nouns in sentences. 
\newcite{yannakoudakis12} represent each sentence by a vector of lemmas and the POS tags of words in sentences, and then the average of the cosine similarities between adjacent sentences encode how coherent the text is.  
\newcite{hearst94, hearst97} computes the cosine similarity between adjacent windows of words, rather than adjacent sentences. 

The third trend of research related to lexical cohesion uses probabilistic models to assign a coherence score to a text. 
\newcite{lapata03} proposes to compute the coherence probability between two adjacent sentences based on their lexical relations. 
This probability for a given pair of sentences is a conditional probability of words in a sentence given all of the words in its immediately preceding sentence. 
The coherence score of a text is the product of coherence probabilities between adjacent sentences. 
Although this model does not use any external knowledge resources, it computes its own co-occurrence matrix representation of a text, which captures the distribution of words across adjacent sentences. 
Moreover, this model learns the order of word pairs in the different type of texts, e.g., \ whether ``CAR'' precedes ``TIRE'' more in coherent texts or in incoherent ones.  
\newcite{lijiwei14} propose to represent words in sentences by pre-trained word embeddings. 
Then a recurrent \cite{schuster97} or a recursive neural network is used to represent each sentence based on its word embeddings. 
A window with length three is sliding through a text, and a coherence probability is computed for every three adjacent sentences.  
The final coherence score of the text is obtained by multiplying these probabilities. 

\section{Applications and Evaluations of Coherence Models}
\label{sec:rel-coh-applications}

Coherence plays a crucial role in different natural language processing applications such as automatic text summarization \cite{celikyilmaz11,linzhiheng12,fengvanessawei12a}, automatic essay scoring \cite{miltsakaki04a,higgins04,burstein10}, readability assessment \cite{pitler08,wangxinhao13}, and so forth.  
Each of these downstream tasks can be employed to evaluate a coherence model. 
In the research of this thesis, we focus on readability assessment and document summarization for evaluating our coherence model. 
Therefore, we review the research related to these two tasks. 


\subsection{Readability Assessment}

Readability is a property of a text, which describes how easily the text can be read and understood.  
\newcite{dale49} define readability as 

\emph{``The sum total (including all the interactions) of all those elements within a given piece of printed material that affect the success a group of readers have with it. 
The success is the extent to which they understand it, read it at optimal speed, and find it interesting''.}

Assessing the degree of readability of a text has been a field of research for many decades. 
Early readability metrics \cite{flesch48,kincaid75} have been established as a function of shallow features of a text, such as the number of syllables per word and the number of words per sentence. 
These traditional readability metrics are still used currently in many settings and domains, mainly because they are very easy and efficient to compute.  

Later research has investigated the use of statistical language models 
(uni-gram in particular) to capture the distribution of vocabulary between two readability grade levels
\cite{siluo01,collins-thompson04}. 
This research trend is followed by an investigation on the effect of syntactic features \cite{schwarm05,heilman07,petersen09} in the assessment of text readability. 
While language model features alone outperform syntactic features in classifying texts according to their reading grade levels, the combination of these two sets of features performed the best. 

However, these features are not sufficient to encode the readability of a text because they never go beyond the level of words and sentences. 
In order to accurately model the difficulty of a text for its readers, besides the surface features some discourse level features are required. 
Indeed, well-written texts are more than unrelated sequences of sentences. 
Discourse-level factors (e.g.\ coherence) of a text play a critical role in the overall understanding of the text \cite{pitler08}.  
In a high-quality text, sentences relate semantically to one another so that they become less ambiguous.   
Moreover, the relationships among sentences of coherent texts are easy to recognize and interpret for readers, and therefore information smoothly moves sentence by sentence as the text progresses. 
\newcite{beigman13} use the lexical relations between words of a text to model the quality of the text. 
The core intuition of their model is that a text segmentation algorithm, which uses information about patterns of word co-occurrences, can detect topic shifts in a text. 
\newcite{eisenstein08} state that coherent texts contain some proportions of more highly associated word pairs (those in its following sentences within the same topical unit) and of less highly associated pairs (those in sentences from different topical units).  
They illustrate that the patterns of the distribution of semantically related words correlate with the writing quality. 
\newcite{pitler08} evaluate coherence features, proposed by the entity grid model, and other discourse features such as the frequency of RST relations for the readability assessment task. 

In order to investigate and compare the impacts of these features on readability assessment, two approaches have been frequently employed. 
The first approach recast readability assessment as a rating task \cite{pitler08,kate10}. 
The requirement for this task is a dataset whose texts accompany ratings assigned by human judges. 
Several human annotators judge each text for its readability by assigning the text a readability rating on an n-point scale, where n is a design choice. 
The average of these ratings is then the final readability rating of the examined text.  
Given such a dataset, a statistical correlation coefficient metric, e.g.\ the Pearson correlation coefficient, between values of a feature and the average human ratings of texts in a corpus is computed to measure which feature is more correlated with human judges.  

The second approach is to distinguish texts which are difficult to read from texts which are easy to read. 
This approach treats the readability assessment task as a pairwise classification task \cite{pitler08,guinaudeau13,barzilay08}: Given a pair of texts, which one is easier to read? 
In this approach, all related features related to coherence and readability are taken into account to increase the predictive power of the classifier. 
However, each feature class is also separately used to classify texts.   
For example, \newcite{pitler08} show that entity transition features introduced by the entity grid model for coherence modeling are the best category of features to classify texts with respect to their readability. 

\subsection{Automatic Text Summarization}

Coherence is a fundamental factor for automatic text generation systems as the output text is supposed to be readable. 
An example of such systems is an automatic text summarization system.
The input to a summarizer is a text (or several texts in the case of multi-document summarization), and the task of the system is to produce a shorter text which contains the gist of the information presented in the input text(s). 
This output text, of course, should be readable and understandable to be used by humans or other natural language processing applications. 

The summarization task has several design choices: single-document vs multi-document, and extractive vs abstractive summarization\footnote{There is another type of summarization which is called compressive summarization, where summaries are formed by compressed sentences not necessarily extracts \cite{knight00}.} \cite{hahn00}. 
Single-document summarization systems take only one input text, whereas multi-document summarizers produce a summary from a cluster of texts. 
Extractive summarizers \cite{kupiec95,carbonell98,gillick09} produce a summary by sequencing a subset of sentences from an input text, while abstractive summarizers \cite{wanglu13b,alfonseca13} involve the generation of sentences for the summary as well.   

The summarization task, in all of its variations, and coherence modeling meet in two general trends. 
The first trend, which is called summary coherence ranking \cite{barzilay08,guinaudeau13}, employs a coherence model to discriminate between pairs of summaries generated by either humans or machines. 
This trend is (almost) similar to the readability assessment task; just examined texts are summaries. 
In this trend, the performance of a coherence model is assessed by comparing rankings induced by the model against rankings elicited by human judges. 
A coherence model that exhibits a high agreement with human judges accurately captures the coherence properties of the texts \cite{barzilay08}. 
Although this approach can potentially be used for evaluating the quality of summaries produced by any summarization system, it is mainly used to compare the outputs of multi-document extractive summarization models.  

The second trend combines coherence metrics with an automatic text summarization system, with the intention of producing coherent summaries.  
This approach is more beneficial for the real application of automatic text summarization. 
For example, some methods \cite{radev04a,nenkova05} use advantages of word repetitions, as a cohesive device, in their \mbox{multi-document} extractive summarizers. 
Some others involve assumptions about the text structure of the input document to a summarizer. 
\newcite{daume02b} hypothesize a hierarchical structure (e.g.\ sections and paragraphs), and \newcite{teufel02} assume a flat structure for input texts. 
\newcite{teufel02} focus on summarizing scientific articles and use lexical cohesion clues in an article for dividing it into research goal (aim), the outline of the paper (textual), presentation of the paper's contribution (methods, results, and discussion), and presentation of other work (other). 
These topical segments are not necessarily consistent with the physical structure of an article and might be distributed evenly through the whole article. 
This strategy is especially fruitful if a summary should contain information about specific parts of a text rather than on the text as a whole.  
\newcite{barzilay97} employ lexical chains occurring in a text to divide the text into segments.  
The use of lexical chains allows topicality to be taken into account to heighten the quality of summaries.
\newcite{celikyilmaz10,celikyilmaz11} incorporate the idea of hierarchical topical coherence models for segmentation and sentence extraction. 
\newcite{clarke10} require the entities that serve as centers of sentences (in the sense of the centering theory) retain in summary. 
\newcite{mckeown99} first initially cluster sentences and then choose the representative sentences of each cluster to be added in the summary. 
Here, the clustering stage minimizes redundancy, and the representative sentence selection maximizes importance.
% \newcite{jha15} consider Minimum Independent Discourse Contexts (MIDC) to solve the problem of non-coherence in extractive summarization.
However, this model does not deal with the problem of coherence within the task of sentence selection. 
\newcite{carbonell98} propose a global model through the use of the Maximum Marginal Relevance (MMR) criteria, where the model scores sentences by considering a weighted combination of importance plus redundancy with sentences already in the summary. 
Summaries are then created with an approximate greedy procedure that incrementally includes the sentence that maximizes this criterion.  
Greedy MMR style algorithms are still a standard baseline for summarization. 

One of the popular approaches to optimizing three main factors for summarization -- importance, variance, and coherence -- is Integer Linear Programming (ILP). 
ILP techniques have been used to solve many intractable inference problems in NLP applications. 
Examples include applications to sentence compression \cite{clarke10,filippova13}, coreference resolution \cite{denis09},  syntactic parsing \cite{klenner07a}, as well as semantic role labeling \cite{punyakanok04b}.
%\cite{marciniak05b,mcdonald07,bergkirkpatrick11,woodsend12,lichen13a,hirao13}.
In an ILP approach, an objective function and some constraints model the optimization problem.  
Solving arbitrary ILPs is an \mbox{NP-hard} problem. 
However, ILPs are well studied resulting in efficient \mbox{branch-and-bound} algorithms for finding an optimal solution. 
Similar to the other NLP applications, ILP has been popularly employed for automatic summarization 
\cite{nishikawa10,galanis12,marciniak05b,mcdonald07,bergkirkpatrick11,woodsend12,lichen13a,hirao13}.  

\newcite{parveen15a} propose an automatic summarizer using ILP.  
The objective of the optimization in their ILP formulation is to select a subset of sentences from the input document so that the selected sentences produce a summary.
The summary should ideally be optimal regarding importance, non-redundancy, and coherence. 
This model is of our interest because it sets up the optimization problem on the entity graph representations of texts. 
Given an input text, the importance and non-redundancy of selected sentences for a summary are measured using the bipartite entity graph of the input text. 
They employ entities as units of information which relate sentences. 
Entities are also used in other summarization techniques to measure the importance and the diversity of the information presented in sentences.   
Sentences that contain prominent entities of a text are essential to be extracted. 
A summary whose sentences refer to few entities may contain redundant information. 
\newcite{parveen15a} use the outdegree of each node in the projection graph representation of the input text in order to measure how important the sentence associated with the node is for the coherence of the summary.  
In Chapter \ref{ch:coh-patterns}, we employ this model to evaluate our coherence patterns by replacing their out degree feature with the frequency of our coherence patterns into this summarizer, instead of their outdegree feature, which is used in this summarizer to measure coherence. 


